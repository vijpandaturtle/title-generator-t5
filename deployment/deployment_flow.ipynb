{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Create Local Sagemaker Session"
      ],
      "metadata": {
        "id": "XMIP0TYnFPm3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzK1ALgeE4qW"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "sess = sagemaker.Session()\n",
        "# sagemaker session bucket -> used for uploading data, models and logs\n",
        "# sagemaker will automatically create this bucket if it not exists\n",
        "sagemaker_session_bucket=None\n",
        "if sagemaker_session_bucket is None and sess is not None:\n",
        "    # set to default bucket if a bucket name is not given\n",
        "    sagemaker_session_bucket = sess.default_bucket()\n",
        "\n",
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    iam = boto3.client('iam')\n",
        "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
        "\n",
        "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
        "\n",
        "print(f\"sagemaker role arn: {role}\")\n",
        "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
        "print(f\"sagemaker session region: {sess.boto_region_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create project assets"
      ],
      "metadata": {
        "id": "Vcnp9_FuFUlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir code"
      ],
      "metadata": {
        "id": "wk8OiFXqFbNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile code/requirements.txt\n",
        "accelerate==0.16.0\n",
        "transformers==4.26.0\n",
        "bitsandbytes==0.37.0"
      ],
      "metadata": {
        "id": "PRA089XVFkLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile code/inference.py\n",
        "from typing import Dict, List, Any\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    # load model and processor from model_dir\n",
        "    model =  AutoModelForSeq2SeqLM.from_pretrained(model_dir, device_map=\"auto\", load_in_8bit=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def predict_fn(data, model_and_tokenizer):\n",
        "    # unpack model and tokenizer\n",
        "    model, tokenizer = model_and_tokenizer\n",
        "\n",
        "    # process input\n",
        "    inputs = data.pop(\"inputs\", data)\n",
        "    parameters = data.pop(\"parameters\", None)\n",
        "\n",
        "    # preprocess\n",
        "    input_ids = tokenizer(inputs, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # pass inputs with all kwargs in data\n",
        "    if parameters is not None:\n",
        "        outputs = model.generate(input_ids, **parameters)\n",
        "    else:\n",
        "        outputs = model.generate(input_ids)\n",
        "\n",
        "    # postprocess the prediction\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return [{\"generated_text\": prediction}]\n"
      ],
      "metadata": {
        "id": "bH-Yt9zlFsNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Step 3: Create SageMaker model.tar.gz artifact"
      ],
      "metadata": {
        "id": "bze0Fv2WF11p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distutils.dir_util import copy_tree\n",
        "from pathlib import Path\n",
        "from tempfile import TemporaryDirectory\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "HF_MODEL_ID=\"philschmid/flan-t5-xxl-sharded-fp16\"\n",
        "# create model dir\n",
        "model_tar_dir = Path(HF_MODEL_ID.split(\"/\")[-1])\n",
        "model_tar_dir.mkdir()\n",
        "\n",
        "# setup temporary directory\n",
        "with TemporaryDirectory() as tmpdir:\n",
        "    # download snapshot\n",
        "    snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID, cache_dir=tmpdir)\n",
        "    # copy snapshot to model dir\n",
        "    copy_tree(snapshot_dir, str(model_tar_dir))"
      ],
      "metadata": {
        "id": "pFnxQmsKFzXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy code/ to model dir\n",
        "copy_tree(\"code/\", str(model_tar_dir.joinpath(\"code\")))"
      ],
      "metadata": {
        "id": "1W_U2ToMMbdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "# helper to create the model.tar.gz\n",
        "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
        "    parent_dir=os.getcwd()\n",
        "    os.chdir(tar_dir)\n",
        "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
        "        for item in os.listdir('.'):\n",
        "          print(item)\n",
        "          tar.add(item, arcname=item)\n",
        "    os.chdir(parent_dir)\n",
        "\n",
        "compress(str(model_tar_dir))"
      ],
      "metadata": {
        "id": "djsKPayzMcFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.s3 import S3Uploader\n",
        "\n",
        "# upload model.tar.gz to s3\n",
        "s3_model_uri = S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/flan-t5-xxl\")\n",
        "\n",
        "print(f\"model uploaded to: {s3_model_uri}\")"
      ],
      "metadata": {
        "id": "NKnYXzHXMfRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Deploy the model of Amazon Sagemaker & run inference"
      ],
      "metadata": {
        "id": "f69YKiLnMmgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.huggingface.model import HuggingFaceModel\n",
        "\n",
        "\n",
        "# create Hugging Face Model Class\n",
        "huggingface_model = HuggingFaceModel(\n",
        "   model_data=s3_model_uri,      # path to your model and script\n",
        "   role=role,                    # iam role with permissions to create an Endpoint\n",
        "   transformers_version=\"4.17\",  # transformers version used\n",
        "   pytorch_version=\"1.10\",       # pytorch version used\n",
        "   py_version='py38',            # python version used\n",
        ")\n",
        "\n",
        "# deploy the endpoint endpoint\n",
        "predictor = huggingface_model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.g5.xlarge\"\n",
        "    )"
      ],
      "metadata": {
        "id": "wHAnom2hMiCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = \"\"\"Summarize the following text:\n",
        "Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.\n",
        "Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.\n",
        "Therefore, Peter stayed with her at the hospital for 3 days without leaving.\n",
        "\"\"\"\n",
        "\n",
        "parameters = {\n",
        "  \"early_stopping\": True,\n",
        "  \"length_penalty\": 2.0,\n",
        "  \"max_new_tokens\": 50,\n",
        "  \"temperature\": 0,\n",
        "  \"min_length\": 10,\n",
        "  \"no_repeat_ngram_size\": 3,\n",
        "}\n",
        "\n",
        "# Run prediction\n",
        "predictor.predict({\n",
        "\t\"inputs\": payload,\n",
        "  \"parameters\" :parameters\n",
        "})\n",
        "# [{'generated_text': 'Peter stayed with Elizabeth at the hospital for 3 days.'}]\n"
      ],
      "metadata": {
        "id": "3wGDMu4HMuAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.delete_model()\n",
        "predictor.delete_endpoint()"
      ],
      "metadata": {
        "id": "jddI7WrcM3AU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}