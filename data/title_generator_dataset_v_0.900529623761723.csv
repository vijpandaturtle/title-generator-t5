,title,summary
0,Watermark-based Detection and Attribution of AI-Generated Content,"Several companies--such as Google, Microsoft, and OpenAI--have deployed
techniques to watermark AI-generated content to enable proactive detection.
However, existing literature mainly focuses on user-agnostic detection.
Attribution aims to further trace back the user of a generative-AI service who
generated a given content detected as AI-generated. Despite its growing
importance, attribution is largely unexplored. In this work, we aim to bridge
this gap by providing the first systematic study on watermark-based, user-aware
detection and attribution of AI-generated content. Specifically, we
theoretically study the detection and attribution performance via rigorous
probabilistic analysis. Moreover, we develop an efficient algorithm to select
watermarks for the users to enhance attribution performance. Both our
theoretical and empirical results show that watermark-based detection and
attribution inherit the accuracy and (non-)robustness properties of the
watermarking method."
1,Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution,"Recent reinforcement learning approaches have shown surprisingly strong
capabilities of bang-bang policies for solving continuous control benchmarks.
The underlying coarse action space discretizations often yield favourable
exploration characteristics while final performance does not visibly suffer in
the absence of action penalization in line with optimal control theory. In
robotics applications, smooth control signals are commonly preferred to reduce
system wear and energy efficiency, but action costs can be detrimental to
exploration during early training. In this work, we aim to bridge this
performance gap by growing discrete action spaces from coarse to fine control
resolution, taking advantage of recent results in decoupled Q-learning to scale
our approach to high-dimensional action spaces up to dim(A) = 38. Our work
indicates that an adaptive control resolution in combination with value
decomposition yields simple critic-only algorithms that yield surprisingly
strong performance on continuous control tasks."
2,Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2),"With advances in the quality of text-to-image (T2I) models has come interest
in benchmarking their prompt faithfulness-the semantic coherence of generated
images to the prompts they were conditioned on. A variety of T2I faithfulness
metrics have been proposed, leveraging advances in cross-modal embeddings and
vision-language models (VLMs). However, these metrics are not rigorously
compared and benchmarked, instead presented against few weak baselines by
correlation to human Likert scores over a set of easy-to-discriminate images.
  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs
containing a prompt and a set increasingly erroneous images. These allow us to
rigorously judge whether a given prompt faithfulness metric can correctly order
images with respect to their objective error count and significantly
discriminate between different error nodes, using meta-metric scores derived
from established statistical tests. Surprisingly, we find that the
state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we
tested fail to significantly outperform simple feature-based metrics like
CLIPScore, particularly on a hard subset of naturally-occurring T2I model
errors. TS2 will enable the development of better T2I prompt faithfulness
metrics through more rigorous comparison of their conformity to expected
orderings and separations under objective criteria."
3,Identity Decoupling for Multi-Subject Personalization of Text-to-Image Models,"Text-to-image diffusion models have shown remarkable success in generating a
personalized subject based on a few reference images. However, current methods
struggle with handling multiple subjects simultaneously, often resulting in
mixed identities with combined attributes from different subjects. In this
work, we present MuDI, a novel framework that enables multi-subject
personalization by effectively decoupling identities from multiple subjects.
Our main idea is to utilize segmented subjects generated by the Segment
Anything Model for both training and inference, as a form of data augmentation
for training and initialization for the generation process. Our experiments
demonstrate that MuDI can produce high-quality personalized images without
identity mixing, even for highly similar subjects as shown in Figure 1. In
human evaluation, MuDI shows twice as many successes for personalizing multiple
subjects without identity mixing over existing baselines and is preferred over
70% compared to the strongest baseline. More results are available at
https://mudi-t2i.github.io/."
4,Physical Property Understanding from Language-Embedded Feature Fields,"Can computers perceive the physical properties of objects solely through
vision? Research in cognitive science and vision science has shown that humans
excel at identifying materials and estimating their physical properties based
purely on visual appearance. In this paper, we present a novel approach for
dense prediction of the physical properties of objects using a collection of
images. Inspired by how humans reason about physics through vision, we leverage
large language models to propose candidate materials for each object. We then
construct a language-embedded point cloud and estimate the physical properties
of each 3D point using a zero-shot kernel regression approach. Our method is
accurate, annotation-free, and applicable to any object in the open world.
Experiments demonstrate the effectiveness of the proposed approach in various
physical property reasoning tasks, such as estimating the mass of common
objects, as well as other properties like friction and hardness."
5,player2vec: A Language Modeling Approach to Understand Player Behavior in Games,"Methods for learning latent user representations from historical behavior
logs have gained traction for recommendation tasks in e-commerce, content
streaming, and other settings. However, this area still remains relatively
underexplored in video and mobile gaming contexts. In this work, we present a
novel method for overcoming this limitation by extending a long-range
Transformer model from the natural language processing domain to player
behavior data. We discuss specifics of behavior tracking in games and propose
preprocessing and tokenization approaches by viewing in-game events in an
analogous way to words in sentences, thus enabling learning player
representations in a self-supervised manner in the absence of ground-truth
annotations. We experimentally demonstrate the efficacy of the proposed
approach in fitting the distribution of behavior events by evaluating intrinsic
language modeling metrics. Furthermore, we qualitatively analyze the emerging
structure of the learned embedding space and show its value for generating
insights into behavior patterns to inform downstream applications."
6,Twins in rotational spectroscopy: Does a rotational spectrum uniquely identify a molecule?,"Rotational spectroscopy is the most accurate method for determining
structures of molecules in the gas phase. It is often assumed that a rotational
spectrum is a unique ""fingerprint"" of a molecule. The availability of large
molecular databases and the development of artificial intelligence methods for
spectroscopy makes the testing of this assumption timely. In this paper, we
pose the determination of molecular structures from rotational spectra as an
inverse problem. Within this framework, we adopt a funnel-based approach to
search for molecular twins, which are two or more molecules, which have similar
rotational spectra but distinctly different molecular structures. We
demonstrate that there are twins within standard levels of computational
accuracy by generating rotational constants for many molecules from several
large molecular databases, indicating the inverse problem is ill-posed.
However, some twins can be distinguished by increasing the accuracy of the
theoretical methods or by performing additional experiments."
7,Active Causal Learning for Decoding Chemical Complexities with Targeted Interventions,"Predicting and enhancing inherent properties based on molecular structures is
paramount to design tasks in medicine, materials science, and environmental
management. Most of the current machine learning and deep learning approaches
have become standard for predictions, but they face challenges when applied
across different datasets due to reliance on correlations between molecular
representation and target properties. These approaches typically depend on
large datasets to capture the diversity within the chemical space, facilitating
a more accurate approximation, interpolation, or extrapolation of the chemical
behavior of molecules. In our research, we introduce an active learning
approach that discerns underlying cause-effect relationships through strategic
sampling with the use of a graph loss function. This method identifies the
smallest subset of the dataset capable of encoding the most information
representative of a much larger chemical space. The identified causal relations
are then leveraged to conduct systematic interventions, optimizing the design
task within a chemical space that the models have not encountered previously.
While our implementation focused on the QM9 quantum-chemical dataset for a
specific design task-finding molecules with a large dipole moment-our active
causal learning approach, driven by intelligent sampling and interventions,
holds potential for broader applications in molecular, materials design and
discovery."
8,Multi-modal perception for soft robotic interactions using generative models,"Perception is essential for the active interaction of physical agents with
the external environment. The integration of multiple sensory modalities, such
as touch and vision, enhances this perceptual process, creating a more
comprehensive and robust understanding of the world. Such fusion is
particularly useful for highly deformable bodies such as soft robots.
Developing a compact, yet comprehensive state representation from multi-sensory
inputs can pave the way for the development of complex control strategies. This
paper introduces a perception model that harmonizes data from diverse
modalities to build a holistic state representation and assimilate essential
information. The model relies on the causality between sensory input and
robotic actions, employing a generative model to efficiently compress fused
information and predict the next observation. We present, for the first time, a
study on how touch can be predicted from vision and proprioception on soft
robots, the importance of the cross-modal generation and why this is essential
for soft robotic interactions in unstructured environments."
9,Continual Policy Distillation of Reinforcement Learning-based Controllers for Soft Robotic In-Hand Manipulation,"Dexterous manipulation, often facilitated by multi-fingered robotic hands,
holds solid impact for real-world applications. Soft robotic hands, due to
their compliant nature, offer flexibility and adaptability during object
grasping and manipulation. Yet, benefits come with challenges, particularly in
the control development for finger coordination. Reinforcement Learning (RL)
can be employed to train object-specific in-hand manipulation policies, but
limiting adaptability and generalizability. We introduce a Continual Policy
Distillation (CPD) framework to acquire a versatile controller for in-hand
manipulation, to rotate different objects in shape and size within a
four-fingered soft gripper. The framework leverages Policy Distillation (PD) to
transfer knowledge from expert policies to a continually evolving student
policy network. Exemplar-based rehearsal methods are then integrated to
mitigate catastrophic forgetting and enhance generalization. The performance of
the CPD framework over various replay strategies demonstrates its effectiveness
in consolidating knowledge from multiple experts and achieving versatile and
adaptive behaviours for in-hand manipulation tasks."
10,Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning Methodology,"The proliferation of the Internet of Things (IoT) has led to an explosion of
data generated by interconnected devices, presenting both opportunities and
challenges for intelligent decision-making in complex environments. Traditional
Reinforcement Learning (RL) approaches often struggle to fully harness this
data due to their limited ability to process and interpret the intricate
patterns and dependencies inherent in IoT applications. This paper introduces a
novel framework that integrates transformer architectures with Proximal Policy
Optimization (PPO) to address these challenges. By leveraging the
self-attention mechanism of transformers, our approach enhances RL agents'
capacity for understanding and acting within dynamic IoT environments, leading
to improved decision-making processes. We demonstrate the effectiveness of our
method across various IoT scenarios, from smart home automation to industrial
control systems, showing marked improvements in decision-making efficiency and
adaptability. Our contributions include a detailed exploration of the
transformer's role in processing heterogeneous IoT data, a comprehensive
evaluation of the framework's performance in diverse environments, and a
benchmark against traditional RL methods. The results indicate significant
advancements in enabling RL agents to navigate the complexities of IoT
ecosystems, highlighting the potential of our approach to revolutionize
intelligent automation and decision-making in the IoT landscape."
11,Probabilistically Informed Robot Object Search with Multiple Regions,"The increasing use of autonomous robot systems in hazardous environments
underscores the need for efficient search and rescue operations. Despite
significant advancements, existing literature on object search often falls
short in overcoming the difficulty of long planning horizons and dealing with
sensor limitations, such as noise. This study introduces a novel approach that
formulates the search problem as a belief Markov decision processes with
options (BMDP-O) to make Monte Carlo tree search (MCTS) a viable tool for
overcoming these challenges in large scale environments. The proposed
formulation incorporates sequences of actions (options) to move between regions
of interest, enabling the algorithm to efficiently scale to large environments.
This approach also enables the use of customizable fields of view, for use with
multiple types of sensors. Experimental results demonstrate the superiority of
this approach in large environments when compared to the problem without
options and alternative tools such as receding horizon planners. Given compute
time for the proposed formulation is relatively high, a further approximated
""lite"" formulation is proposed. The lite formulation finds objects in a
comparable number of steps with faster computation."
12,Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model,"In this study, we introduce CT-LLM, a 2B large language model (LLM) that
illustrates a pivotal shift towards prioritizing the Chinese language in
developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the
conventional methodology by primarily incorporating Chinese textual data,
utilizing an extensive corpus of 1,200 billion tokens, including 800 billion
Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This
strategic composition facilitates the model's exceptional proficiency in
understanding and processing Chinese, a capability further enhanced through
alignment techniques. Demonstrating remarkable performance on the CHC-Bench,
CT-LLM excels in Chinese language tasks, and showcases its adeptness in English
through SFT. This research challenges the prevailing paradigm of training LLMs
predominantly on English corpora and then adapting them to other languages,
broadening the horizons for LLM training methodologies. By open-sourcing the
full process of training a Chinese LLM, including a detailed data processing
procedure with the obtained Massive Appropriate Pretraining Chinese Corpus
(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark
(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster
further exploration and innovation in both academia and industry, paving the
way for more inclusive and versatile language models."
13,Noisy Label Processing for Classification: A Survey,"In recent years, deep neural networks (DNNs) have gained remarkable
achievement in computer vision tasks, and the success of DNNs often depends
greatly on the richness of data. However, the acquisition process of data and
high-quality ground truth requires a lot of manpower and money. In the long,
tedious process of data annotation, annotators are prone to make mistakes,
resulting in incorrect labels of images, i.e., noisy labels. The emergence of
noisy labels is inevitable. Moreover, since research shows that DNNs can easily
fit noisy labels, the existence of noisy labels will cause significant damage
to the model training process. Therefore, it is crucial to combat noisy labels
for computer vision tasks, especially for classification tasks. In this survey,
we first comprehensively review the evolution of different deep learning
approaches for noisy label combating in the image classification task. In
addition, we also review different noise patterns that have been proposed to
design robust algorithms. Furthermore, we explore the inner pattern of
real-world label noise and propose an algorithm to generate a synthetic label
noise pattern guided by real-world data. We test the algorithm on the
well-known real-world dataset CIFAR-10N to form a new real-world data-guided
synthetic benchmark and evaluate some typical noise-robust methods on the
benchmark."
14,Precision Guided Approach to Mitigate Data Poisoning Attacks in Federated Learning,"Federated Learning (FL) is a collaborative learning paradigm enabling
participants to collectively train a shared machine learning model while
preserving the privacy of their sensitive data. Nevertheless, the inherent
decentralized and data-opaque characteristics of FL render its susceptibility
to data poisoning attacks. These attacks introduce malformed or malicious
inputs during local model training, subsequently influencing the global model
and resulting in erroneous predictions. Current FL defense strategies against
data poisoning attacks either involve a trade-off between accuracy and
robustness or necessitate the presence of a uniformly distributed root dataset
at the server. To overcome these limitations, we present FedZZ, which harnesses
a zone-based deviating update (ZBDU) mechanism to effectively counter data
poisoning attacks in FL. Further, we introduce a precision-guided methodology
that actively characterizes these client clusters (zones), which in turn aids
in recognizing and discarding malicious updates at the server. Our evaluation
of FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrate
its efficacy in mitigating data poisoning attacks, surpassing the performance
of prevailing state-of-the-art methodologies in both single and multi-client
attack scenarios and varying attack volumes. Notably, FedZZ also functions as a
robust client selection strategy, even in highly non-IID and attack-free
scenarios. Moreover, in the face of escalating poisoning rates, the model
accuracy attained by FedZZ displays superior resilience compared to existing
techniques. For instance, when confronted with a 50% presence of malicious
clients, FedZZ sustains an accuracy of 67.43%, while the accuracy of the
second-best solution, FL-Defender, diminishes to 43.36%."
15,Large language models as oracles for instantiating ontologies with domain-specific knowledge,"Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes andproperties and (ii) a set of query
templates, our method queries the LLM multi- ple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
semi-automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Finally, we provide a SWOT analysis of the proposed method."
16,Intervention-Assisted Policy Gradient Methods for Online Stochastic Queuing Network Optimization: Technical Report,"Deep Reinforcement Learning (DRL) offers a powerful approach to training
neural network control policies for stochastic queuing networks (SQN). However,
traditional DRL methods rely on offline simulations or static datasets,
limiting their real-world application in SQN control. This work proposes Online
Deep Reinforcement Learning-based Controls (ODRLC) as an alternative, where an
intelligent agent interacts directly with a real environment and learns an
optimal control policy from these online interactions. SQNs present a challenge
for ODRLC due to the unbounded nature of the queues within the network
resulting in an unbounded state-space. An unbounded state-space is particularly
challenging for neural network policies as neural networks are notoriously poor
at extrapolating to unseen states. To address this challenge, we propose an
intervention-assisted framework that leverages strategic interventions from
known stable policies to ensure the queue sizes remain bounded. This framework
combines the learning power of neural networks with the guaranteed stability of
classical control policies for SQNs. We introduce a method to design these
intervention-assisted policies to ensure strong stability of the network.
Furthermore, we extend foundational DRL theorems for intervention-assisted
policies and develop two practical algorithms specifically for ODRLC of SQNs.
Finally, we demonstrate through experiments that our proposed algorithms
outperform both classical control approaches and prior ODRLC algorithms."
17,Robust Preference Optimization with Provable Noise Tolerance for LLMs,"The preference alignment aims to enable large language models (LLMs) to
generate responses that conform to human values, which is essential for
developing general AI systems. Ranking-based methods -- a promising class of
alignment approaches -- learn human preferences from datasets containing
response pairs by optimizing the log-likelihood margins between preferred and
dis-preferred responses. However, due to the inherent differences in
annotators' preferences, ranking labels of comparisons for response pairs are
unavoidably noisy. This seriously hurts the reliability of existing
ranking-based methods. To address this problem, we propose a provably
noise-tolerant preference alignment method, namely RObust Preference
Optimization (ROPO). To the best of our knowledge, ROPO is the first preference
alignment method with noise-tolerance guarantees. The key idea of ROPO is to
dynamically assign conservative gradient weights to response pairs with high
label uncertainty, based on the log-likelihood margins between the responses.
By effectively suppressing the gradients of noisy samples, our weighting
strategy ensures that the expected risk has the same gradient direction
independent of the presence and proportion of noise. Experiments on three
open-ended text generation tasks with four base models ranging in size from
2.8B to 13B demonstrate that ROPO significantly outperforms existing
ranking-based methods."
18,Dynamic Prompt Optimizing for Text-to-Image Generation,"Text-to-image generative models, specifically those based on diffusion models
like Imagen and Stable Diffusion, have made substantial advancements. Recently,
there has been a surge of interest in the delicate refinement of text prompts.
Users assign weights or alter the injection time steps of certain words in the
text prompts to improve the quality of generated images. However, the success
of fine-control prompts depends on the accuracy of the text prompts and the
careful selection of weights and time steps, which requires significant manual
intervention. To address this, we introduce the \textbf{P}rompt
\textbf{A}uto-\textbf{E}diting (PAE) method. Besides refining the original
prompts for image generation, we further employ an online reinforcement
learning strategy to explore the weights and injection time steps of each word,
leading to the dynamic fine-control prompts. The reward function during
training encourages the model to consider aesthetic score, semantic
consistency, and user preferences. Experimental results demonstrate that our
proposed method effectively improves the original prompts, generating visually
more appealing images while maintaining semantic alignment. Code is available
at https://github.com/Mowenyii/PAE."
19,Self-Sensing Feedback Control of an Electrohydraulic Robotic Shoulder,"The human shoulder, with its glenohumeral joint, tendons, ligaments, and
muscles, allows for the execution of complex tasks with precision and
efficiency. However, current robotic shoulder designs lack the compliance and
compactness inherent in their biological counterparts. A major limitation of
these designs is their reliance on external sensors like rotary encoders, which
restrict mechanical joint design and introduce bulk to the system. To address
this constraint, we present a bio-inspired antagonistic robotic shoulder with
two degrees of freedom powered by self-sensing hydraulically amplified
self-healing electrostatic actuators. Our artificial muscle design decouples
the high-voltage electrostatic actuation from the pair of low-voltage
self-sensing electrodes. This approach allows for proprioceptive feedback
control of trajectories in the task space while eliminating the necessity for
any additional sensors. We assess the platform's efficacy by comparing it to a
feedback control based on position data provided by a motion capture system.
The study demonstrates closed-loop controllable robotic manipulators based on
an inherent self-sensing capability of electrohydraulic actuators. The proposed
architecture can serve as a basis for complex musculoskeletal joint
arrangements."
20,CLUE: A Clinical Language Understanding Evaluation for LLMs,"Large Language Models (LLMs) have shown the potential to significantly
contribute to patient care, diagnostics, and administrative processes. Emerging
biomedical LLMs address healthcare-specific challenges, including privacy
demands and computational constraints. However, evaluation of these models has
primarily been limited to non-clinical tasks, which do not reflect the
complexity of practical clinical applications. Additionally, there has been no
thorough comparison between biomedical and general-domain LLMs for clinical
tasks. To fill this gap, we present the Clinical Language Understanding
Evaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical
tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters
and four existing tasks designed to test the practical applicability of LLMs in
healthcare settings. Our evaluation covers several biomedical and general
domain LLMs, providing insights into their clinical performance and
applicability. CLUE represents a step towards a standardized approach to
evaluating and developing LLMs in healthcare to align future model development
with the real-world needs of clinical application. We publish our evaluation
and data generation scripts: https://github.com/dadaamin/CLUE"
21,Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation,"We introduce Score identity Distillation (SiD), an innovative data-free
method that distills the generative capabilities of pretrained diffusion models
into a single-step generator. SiD not only facilitates an exponentially fast
reduction in Fr\'echet inception distance (FID) during distillation but also
approaches or even exceeds the FID performance of the original teacher
diffusion models. By reformulating forward diffusion processes as semi-implicit
distributions, we leverage three score-related identities to create an
innovative loss mechanism. This mechanism achieves rapid FID reduction by
training the generator using its own synthesized images, eliminating the need
for real data or reverse-diffusion-based generation, all accomplished within
significantly shortened generation time. Upon evaluation across four benchmark
datasets, the SiD algorithm demonstrates high iteration efficiency during
distillation and surpasses competing distillation approaches, whether they are
one-step or few-step, data-free, or dependent on training data, in terms of
generation quality. This achievement not only redefines the benchmarks for
efficiency and effectiveness in diffusion distillation but also in the broader
field of diffusion-based generation. Our PyTorch implementation will be
publicly accessible on GitHub."
22,Dynamic Risk Assessment Methodology with an LDM-based System for Parking Scenarios,"This paper describes the methodology for building a dynamic risk assessment
for ADAS (Advanced Driving Assistance Systems) algorithms in parking scenarios,
fusing exterior and interior perception for a better understanding of the scene
and a more comprehensive risk estimation. This includes the definition of a
dynamic risk methodology that depends on the situation from inside and outside
the vehicle, the creation of a multi-sensor dataset of risk assessment for ADAS
benchmarking purposes, and a Local Dynamic Map (LDM) that fuses data from the
exterior and interior of the car to build an LDM-based Dynamic Risk Assessment
System (DRAS)."
23,Validation of critical maneuvers based on shared control,"This paper presents the validation of shared control strategies for critical
maneuvers in automated driving systems. Shared control involves collaboration
between the driver and automation, allowing both parties to actively engage and
cooperate at different levels of the driving task. The involvement of the
driver adds complexity to the control loop, necessitating comprehensive
validation methodologies. The proposed approach focuses on two critical
maneuvers: overtaking in low visibility scenarios and lateral evasive actions.
A modular architecture with an arbitration module and shared control algorithms
is implemented, primarily focusing on the lateral control of the vehicle. The
validation is conducted using a dynamic simulator, involving 8 real drivers
interacting with a virtual environment. The results demonstrate improved safety
and user acceptance, indicating the effectiveness of the shared control
strategies in comparison with no shared-control support. Future work involves
implementing shared control in drive-by-wire systems to enhance safety and
driver comfort during critical maneuvers. Overall, this research contributes to
the development and validation of shared control approaches in automated
driving systems."
24,Approximate UMAP allows for high-rate online visualization of high-dimensional data streams,"In the BCI field, introspection and interpretation of brain signals are
desired for providing feedback or to guide rapid paradigm prototyping but are
challenging due to the high noise level and dimensionality of the signals. Deep
neural networks are often introspected by transforming their learned feature
representations into 2- or 3-dimensional subspace visualizations using
projection algorithms like Uniform Manifold Approximation and Projection
(UMAP). Unfortunately, these methods are computationally expensive, making the
projection of data streams in real-time a non-trivial task. In this study, we
introduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at
generating rapid projections for real-time introspection. To study its
suitability for real-time projecting, we benchmark the methods against standard
UMAP and its neural network counterpart parametric UMAP. Our results show that
approximate UMAP delivers projections that replicate the projection space of
standard UMAP while decreasing projection speed by an order of magnitude and
maintaining the same training time."
25,Demonstration Guided Multi-Objective Reinforcement Learning,"Multi-objective reinforcement learning (MORL) is increasingly relevant due to
its resemblance to real-world scenarios requiring trade-offs between multiple
objectives. Catering to diverse user preferences, traditional reinforcement
learning faces amplified challenges in MORL. To address the difficulty of
training policies from scratch in MORL, we introduce demonstration-guided
multi-objective reinforcement learning (DG-MORL). This novel approach utilizes
prior demonstrations, aligns them with user preferences via corner weight
support, and incorporates a self-evolving mechanism to refine suboptimal
demonstrations. Our empirical studies demonstrate DG-MORL's superiority over
existing MORL algorithms, establishing its robustness and efficacy,
particularly under challenging conditions. We also provide an upper bound of
the algorithm's sample complexity."
26,Fast Genetic Algorithm for feature selection -- A qualitative approximation approach,"Evolutionary Algorithms (EAs) are often challenging to apply in real-world
settings since evolutionary computations involve a large number of evaluations
of a typically expensive fitness function. For example, an evaluation could
involve training a new machine learning model. An approximation (also known as
meta-model or a surrogate) of the true function can be used in such
applications to alleviate the computation cost. In this paper, we propose a
two-stage surrogate-assisted evolutionary approach to address the computational
issues arising from using Genetic Algorithm (GA) for feature selection in a
wrapper setting for large datasets. We define 'Approximation Usefulness' to
capture the necessary conditions to ensure correctness of the EA computations
when an approximation is used. Based on this definition, we propose a procedure
to construct a lightweight qualitative meta-model by the active selection of
data instances. We then use a meta-model to carry out the feature selection
task. We apply this procedure to the GA-based algorithm CHC (Cross generational
elitist selection, Heterogeneous recombination and Cataclysmic mutation) to
create a Qualitative approXimations variant, CHCQX. We show that CHCQX
converges faster to feature subset solutions of significantly higher accuracy
(as compared to CHC), particularly for large datasets with over 100K instances.
We also demonstrate the applicability of the thinking behind our approach more
broadly to Swarm Intelligence (SI), another branch of the Evolutionary
Computation (EC) paradigm with results of PSOQX, a qualitative approximation
adaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository
with the complete implementation is available."
27,Balancing Progress and Responsibility: A Synthesis of Sustainability Trade-Offs of AI-Based Systems,"Recent advances in artificial intelligence (AI) capabilities have increased
the eagerness of companies to integrate AI into software systems. While AI can
be used to have a positive impact on several dimensions of sustainability, this
is often overshadowed by its potential negative influence. While many studies
have explored sustainability factors in isolation, there is insufficient
holistic coverage of potential sustainability benefits or costs that
practitioners need to consider during decision-making for AI adoption. We
therefore aim to synthesize trade-offs related to sustainability in the context
of integrating AI into software systems. We want to make the sustainability
benefits and costs of integrating AI more transparent and accessible for
practitioners.
  The study was conducted in collaboration with a Dutch financial organization.
We first performed a rapid review that led to the inclusion of 151 research
papers. Afterward, we conducted six semi-structured interviews to enrich the
data with industry perspectives. The combined results showcase the potential
sustainability benefits and costs of integrating AI. The labels synthesized
from the review regarding potential sustainability benefits were clustered into
16 themes, with ""energy management"" being the most frequently mentioned one. 11
themes were identified in the interviews, with the top mentioned theme being
""employee wellbeing"". Regarding sustainability costs, the review discovered
seven themes, with ""deployment issues"" being the most popular one, followed by
""ethics & society"". ""Environmental issues"" was the top theme from the
interviews. Our results provide valuable insights to organizations and
practitioners for understanding the potential sustainability implications of
adopting AI."
28,Rolling the dice for better deep learning performance: A study of randomness techniques in deep neural networks,"This paper investigates how various randomization techniques impact Deep
Neural Networks (DNNs). Randomization, like weight noise and dropout, aids in
reducing overfitting and enhancing generalization, but their interactions are
poorly understood. The study categorizes randomness techniques into four types
and proposes new methods: adding noise to the loss function and random masking
of gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter
optimization, it explores optimal configurations across MNIST, FASHION-MNIST,
CIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated,
revealing data augmentation and weight initialization randomness as main
performance contributors. Correlation analysis shows different optimizers
prefer distinct randomization types. The complete implementation and dataset
are available on GitHub."
29,Random Walk in Random Permutation Set Theory,"Random walk is an explainable approach for modeling natural processes at the
molecular level. The Random Permutation Set Theory (RPST) serves as a framework
for uncertainty reasoning, extending the applicability of Dempster-Shafer
Theory. Recent explorations indicate a promising link between RPST and random
walk. In this study, we conduct an analysis and construct a random walk model
based on the properties of RPST, with Monte Carlo simulations of such random
walk. Our findings reveal that the random walk generated through RPST exhibits
characteristics similar to those of a Gaussian random walk and can be
transformed into a Wiener process through a specific limiting scaling
procedure. This investigation establishes a novel connection between RPST and
random walk theory, thereby not only expanding the applicability of RPST, but
also demonstrating the potential for combining the strengths of both approaches
to improve problem-solving abilities."
30,On the Antiferromagnetic$\unicode{x2013}$Ferromagnetic Phase Transition in Pinwheel Artificial Spin Ice,"Nanopatterned magnetic thin films offer a platform for exploration of
tailored magnetic properties such as emergent long-range order. A prominent
example is artificial spin ice (ASI), where an arrangement of nanoscale
magnetic elements, acting as macrospins, interact via their dipolar fields. In
this study, we discuss the transition from antiferromagnetic (AF) to
ferromagnetic (FM) long-range order in a square lattice ASI, as the magnetic
elements are gradually rotated through 45{\deg} to a ""pinwheel"" configuration.
The AF$\unicode{x2013}$FM transition is observed experimentally using
synchrotron radiation x-ray spectromicroscopy and occurs for a critical
rotation angle of the nanomagnets, contingent upon the dipolar coupling
determined by their separation in the lattice. Large-scale magnetic dipole
simulations show that the point-dipole approximation fails to capture the
correct AF$\unicode{x2013}$FM transition angle. However, excellent agreement
with experimental data is obtained using a dumbbell-dipole model which better
reflects the actual dipolar fields of the magnets. This model explains the
coupling-dependence of the transition angle, another feature not captured by
the pointdipole model. Our findings resolve a discrepancy between measurement
and theory in previous work on ""pinwheel"" ASIs. Control of the
AF$\unicode{x2013}$FM transition and this revised model open for improved
design of magnetic order in nanostructured systems."
31,Bayesian Graphs of Intelligent Causation,"Probabilistic Graphical Bayesian models of causation have continued to impact
on strategic analyses designed to help evaluate the efficacy of different
interventions on systems. However, the standard causal algebras upon which
these inferences are based typically assume that the intervened population does
not react intelligently to frustrate an intervention. In an adversarial setting
this is rarely an appropriate assumption. In this paper, we extend an
established Bayesian methodology called Adversarial Risk Analysis to apply it
to settings that can legitimately be designated as causal in this graphical
sense. To embed this technology we first need to generalize the concept of a
causal graph. We then proceed to demonstrate how the predicable intelligent
reactions of adversaries to circumvent an intervention when they hear about it
can be systematically modelled within such graphical frameworks, importing
these recent developments from Bayesian game theory. The new methodologies and
supporting protocols are illustrated through applications associated with an
adversary attempting to infiltrate a friendly state."
32,Artificial Relaxation in NMR Experiment,"Environmental noises cause the relaxation of quantum systems and decrease the
precision of operations. Apprehending the relaxation mechanism via
environmental noises is essential for building quantum technologies.
Relaxations can be considered a process of information dissipation from the
system into an environment with infinite degrees of freedom (DoF). According to
this idea, a model of artificial relaxation has been proposed and demonstrated
in NMR experiments. Although this model successfully understood the central
idea of relaxation, we observed recursive behavior, which is non-ideal to
describe relaxation, because of few DoF of the ``artificial environment''. In
this paper, we extend the approach of the artificial environment and discuss,
theoretically and experimentally, how many DoF of the environment are necessary
for realizing ideal relaxation behavior. Our approach will help us thoroughly
understand the concept of relaxation."
33,Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models,"While there has been significant progress in customizing text-to-image
generation models, generating images that combine multiple personalized
concepts remains challenging. In this work, we introduce Concept Weaver, a
method for composing customized text-to-image diffusion models at inference
time. Specifically, the method breaks the process into two steps: creating a
template image aligned with the semantics of input prompts, and then
personalizing the template using a concept fusion strategy. The fusion strategy
incorporates the appearance of the target concepts into the template image
while retaining its structural details. The results indicate that our method
can generate multiple custom concepts with higher identity fidelity compared to
alternative approaches. Furthermore, the method is shown to seamlessly handle
more than two concepts and closely follow the semantic meaning of the input
prompt without blending appearances across different subjects."
34,"Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for Low-Resource Languages with Application to Luxembourgish","In NLP, zero-shot classification (ZSC) is the task of assigning labels to
textual data without any labeled examples for the target classes. A common
method for ZSC is to fine-tune a language model on a Natural Language Inference
(NLI) dataset and then use it to infer the entailment between the input
document and the target labels. However, this approach faces certain
challenges, particularly for languages with limited resources. In this paper,
we propose an alternative solution that leverages dictionaries as a source of
data for ZSC. We focus on Luxembourgish, a low-resource language spoken in
Luxembourg, and construct two new topic relevance classification datasets based
on a dictionary that provides various synonyms, word translations and example
sentences. We evaluate the usability of our dataset and compare it with the
NLI-based approach on two topic classification tasks in a zero-shot manner. Our
results show that by using the dictionary-based dataset, the trained models
outperform the ones following the NLI-based approach for ZSC. While we focus on
a single low-resource language in this study, we believe that the efficacy of
our approach can also transfer to other languages where such a dictionary is
available."
35,Multi-Task Learning for Lung sound & Lung disease classification,"In recent years, advancements in deep learning techniques have considerably
enhanced the efficiency and accuracy of medical diagnostics. In this work, a
novel approach using multi-task learning (MTL) for the simultaneous
classification of lung sounds and lung diseases is proposed. Our proposed model
leverages MTL with four different deep learning models such as 2D CNN,
ResNet50, MobileNet and Densenet to extract relevant features from the lung
sound recordings. The ICBHI 2017 Respiratory Sound Database was employed in the
current study. The MTL for MobileNet model performed better than the other
models considered, with an accuracy of74\% for lung sound analysis and 91\% for
lung diseases classification. Results of the experimentation demonstrate the
efficacy of our approach in classifying both lung sounds and lung diseases
concurrently.
  In this study,using the demographic data of the patients from the database,
risk level computation for Chronic Obstructive Pulmonary Disease is also
carried out. For this computation, three machine learning algorithms namely
Logistic Regression, SVM and Random Forest classifierswere employed. Among
these ML algorithms, the Random Forest classifier had the highest accuracy of
92\%.This work helps in considerably reducing the physician's burden of not
just diagnosing the pathology but also effectively communicating to the patient
about the possible causes or outcomes."
36,Nonparametric Modern Hopfield Models,"We present a nonparametric construction for deep learning compatible modern
Hopfield models and utilize this framework to debut an efficient variant. Our
key contribution stems from interpreting the memory storage and retrieval
processes in modern Hopfield models as a nonparametric regression problem
subject to a set of query-memory pairs. Crucially, our framework not only
recovers the known results from the original dense modern Hopfield model but
also fills the void in the literature regarding efficient modern Hopfield
models, by introducing \textit{sparse-structured} modern Hopfield models with
sub-quadratic complexity. We establish that this sparse model inherits the
appealing theoretical properties of its dense analogue -- connection with
transformer attention, fixed point convergence and exponential memory capacity
-- even without knowing details of the Hopfield energy function. Additionally,
we showcase the versatility of our framework by constructing a family of modern
Hopfield models as extensions, including linear, random masked, top-$K$ and
positive random feature modern Hopfield models. Empirically, we validate the
efficacy of our framework in both synthetic and realistic settings."
37,Holon: a cybernetic interface for bio-semiotics,"This paper presents an interactive artwork, ""Holon"", a collection of 130
autonomous, cybernetic organisms that listen and make sound in collaboration
with the natural environment. The work was developed for installation on water
at a heritage-listed dock in Melbourne, Australia. Conceptual issues informing
the work are presented, along with a detailed technical overview of the
implementation. Individual holons are of three types, inspired by biological
models of animal communication: composer/generators, collector/critics and
disruptors. Collectively, Holon integrates and occupies elements of the
acoustic spectrum in collaboration with human and non-human agents."
38,KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion,"Knowledge graph completion (KGC) aims to alleviate the inherent
incompleteness of knowledge graphs (KGs), which is a critical task for various
applications, such as recommendations on the web. Although knowledge graph
embedding (KGE) models have demonstrated superior predictive performance on KGC
tasks, these models infer missing links in a black-box manner that lacks
transparency and accountability, preventing researchers from developing
accountable models. Existing KGE-based explanation methods focus on exploring
key paths or isolated edges as explanations, which is information-less to
reason target prediction. Additionally, the missing ground truth leads to these
explanation methods being ineffective in quantitatively evaluating explored
explanations. To overcome these limitations, we propose KGExplainer, a
model-agnostic method that identifies connected subgraph explanations and
distills an evaluator to assess them quantitatively. KGExplainer employs a
perturbation-based greedy search algorithm to find key connected subgraphs as
explanations within the local structure of target predictions. To evaluate the
quality of the explored explanations, KGExplainer distills an evaluator from
the target KGE model. By forwarding the explanations to the evaluator, our
method can examine the fidelity of them. Extensive experiments on benchmark
datasets demonstrate that KGExplainer yields promising improvement and achieves
an optimal ratio of 83.3% in human evaluation."
39,Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI,"The study introduces an integrated framework combining Convolutional Neural
Networks (CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced
diagnosis of breast cancer using the CBIS-DDSM dataset. Utilizing a fine-tuned
ResNet50 architecture, our investigation not only provides effective
differentiation of mammographic images into benign and malignant categories but
also addresses the opaque ""black-box"" nature of deep learning models by
employing XAI methodologies, namely Grad-CAM, LIME, and SHAP, to interpret CNN
decision-making processes for healthcare professionals. Our methodology
encompasses an elaborate data preprocessing pipeline and advanced data
augmentation techniques to counteract dataset limitations, and transfer
learning using pre-trained networks, such as VGG-16, DenseNet and ResNet was
employed. A focal point of our study is the evaluation of XAI's effectiveness
in interpreting model predictions, highlighted by utilising the Hausdorff
measure to assess the alignment between AI-generated explanations and expert
annotations quantitatively. This approach plays a critical role for XAI in
promoting trustworthiness and ethical fairness in AI-assisted diagnostics. The
findings from our research illustrate the effective collaboration between CNNs
and XAI in advancing diagnostic methods for breast cancer, thereby facilitating
a more seamless integration of advanced AI technologies within clinical
settings. By enhancing the interpretability of AI-driven decisions, this work
lays the groundwork for improved collaboration between AI systems and medical
practitioners, ultimately enriching patient care. Furthermore, the implications
of our research extend well beyond the current methodologies, advocating for
subsequent inquiries into the integration of multimodal data and the refinement
of AI explanations to satisfy the needs of clinical practice."
40,Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning,"In robotics, the use of Large Language Models (LLMs) is becoming prevalent,
especially for understanding human commands. In particular, LLMs are utilized
as domain-agnostic task planners for high-level human commands. LLMs are
capable of Chain-of-Thought (CoT) reasoning, and this allows LLMs to be task
planners. However, we need to consider that modern robots still struggle to
perform complex actions, and the domains where robots can be deployed are
limited in practice. This leads us to pose a question: If small LMs can be
trained to reason in chains within a single domain, would even small LMs be
good task planners for the robots? To train smaller LMs to reason in chains, we
build `COmmand-STeps datasets' (COST) consisting of high-level commands along
with corresponding actionable low-level steps, via LLMs. We release not only
our datasets but also the prompt templates used to generate them, to allow
anyone to build datasets for their domain. We compare GPT3.5 and GPT4 with the
finetuned GPT2 for task domains, in tabletop and kitchen environments, and the
result shows that GPT2-medium is comparable to GPT3.5 for task planning in a
specific domain. Our dataset, code, and more output samples can be found in
https://github.com/Gawon-Choi/small-LMs-Task-Planning"
41,A proximal policy optimization based intelligent home solar management,"In the smart grid, the prosumers can sell unused electricity back to the
power grid, assuming the prosumers own renewable energy sources and storage
units. The maximizing of their profits under a dynamic electricity market is a
problem that requires intelligent planning. To address this, we propose a
framework based on Proximal Policy Optimization (PPO) using recurrent rewards.
By using the information about the rewards modeled effectively with PPO to
maximize our objective, we were able to get over 30\% improvement over the
other naive algorithms in accumulating total profits. This shows promise in
getting reinforcement learning algorithms to perform tasks required to plan
their actions in complex domains like financial markets. We also introduce a
novel method for embedding longs based on soliton waves that outperformed
normal embedding in our use case with random floating point data augmentation."
42,SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical Reasoning in Large Language Models,"This study presents a novel learning approach designed to enhance both
mathematical reasoning and problem-solving abilities of Large Language Models
(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the
Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning
of mathematical reasoning ability is helpful for the amplification of
problem-solving ability. Thus, the initial learning with CoT is essential for
solving challenging mathematical problems. To this end, we propose a sequential
learning approach, named SAAS (Solving Ability Amplification Strategy), which
strategically transitions from CoT learning to PoT learning. Our empirical
study, involving an extensive performance comparison using several benchmarks,
demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The
results underscore the effectiveness of our sequential learning approach,
marking a significant advancement in the field of mathematical reasoning in
LLMs."
43,Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable Collaboration,"The rise of multi-agent systems, especially the success of multi-agent
reinforcement learning (MARL), is reshaping our future across diverse domains
like autonomous vehicle networks. However, MARL still faces significant
challenges, particularly in achieving zero-shot scalability, which allows
trained MARL models to be directly applied to unseen tasks with varying numbers
of agents. In addition, real-world multi-agent systems usually contain agents
with different functions and strategies, while the existing scalable MARL
methods only have limited heterogeneity. To address this, we propose a novel
MARL framework named Scalable and Heterogeneous Proximal Policy Optimization
(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL
networks. we first leverage a latent network to adaptively learn strategy
patterns for each agent. Second, we introduce a heterogeneous layer for
decision-making, whose parameters are specifically generated by the learned
latent variables. Our approach is scalable as all the parameters are shared
except for the heterogeneous layer, and gains both inter-individual and
temporal heterogeneity at the same time. We implement our approach based on the
state-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is
agnostic to the backbone and can be seamlessly plugged into any
parameter-shared MARL method. SHPPO exhibits superior performance over the
baselines such as MAPPO and HAPPO in classic MARL environments like Starcraft
Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing
enhanced zero-shot scalability and offering insights into the learned latent
representation's impact on team performance by visualization."
44,"Extract, Define, Canonicalize: An LLM-based Framework for Knowledge Graph Construction","In this work, we are interested in automated methods for knowledge graph
creation (KGC) from input text. Progress on large language models (LLMs) has
prompted a series of recent works applying them to KGC, e.g., via zero/few-shot
prompting. Despite successes on small domain-specific datasets, these models
face difficulties scaling up to text common in many real-world applications. A
principal issue is that in prior methods, the KG schema has to be included in
the LLM prompt to generate valid triplets; larger and more complex schema
easily exceed the LLMs' context window length. To address this problem, we
propose a three-phase framework named Extract-Define-Canonicalize (EDC): open
information extraction followed by schema definition and post-hoc
canonicalization. EDC is flexible in that it can be applied to settings where a
pre-defined target schema is available and when it is not; in the latter case,
it constructs a schema automatically and applies self-canonicalization. To
further improve performance, we introduce a trained component that retrieves
schema elements relevant to the input text; this improves the LLMs' extraction
performance in a retrieval-augmented generation-like manner. We demonstrate on
three KGC benchmarks that EDC is able to extract high-quality triplets without
any parameter tuning and with significantly larger schemas compared to prior
works."
45,A Block-Coordinate Descent EMO Algorithm: Theoretical and Empirical Analysis,"We consider whether conditions exist under which block-coordinate descent is
asymptotically efficient in evolutionary multi-objective optimization,
addressing an open problem. Block-coordinate descent, where an optimization
problem is decomposed into $k$ blocks of decision variables and each of the
blocks is optimized (with the others fixed) in a sequence, is a technique used
in some large-scale optimization problems such as airline scheduling, however
its use in multi-objective optimization is less studied. We propose a
block-coordinate version of GSEMO and compare its running time to the standard
GSEMO algorithm. Theoretical and empirical results on a bi-objective test
function, a variant of LOTZ, serve to demonstrate the existence of cases where
block-coordinate descent is faster. The result may yield wider insights into
this class of algorithms."
46,PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal Model,"Recent advancements in 3D perception systems have significantly improved
their ability to perform visual recognition tasks such as segmentation.
However, these systems still heavily rely on explicit human instruction to
identify target objects or categories, lacking the capability to actively
reason and comprehend implicit user intentions. We introduce a novel
segmentation task known as reasoning part segmentation for 3D objects, aiming
to output a segmentation mask based on complex and implicit textual queries
about specific parts of a 3D object. To facilitate evaluation and benchmarking,
we present a large 3D dataset comprising over 60k instructions paired with
corresponding ground-truth part segmentation annotations specifically curated
for reasoning-based 3D part segmentation. We propose a model that is capable of
segmenting parts of 3D objects based on implicit textual queries and generating
natural language explanations corresponding to 3D object segmentation requests.
Experiments show that our method achieves competitive performance to models
that use explicit queries, with the additional abilities to identify part
concepts, reason about them, and complement them with world knowledge. Our
source code, dataset, and trained models are available at
https://github.com/AmrinKareem/PARIS3D."
47,BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model,"We introduce the \textbf{B}i-Directional \textbf{S}parse \textbf{Hop}field
Network (\textbf{BiSHop}), a novel end-to-end framework for deep tabular
learning. BiSHop handles the two major challenges of deep tabular learning:
non-rotationally invariant data structure and feature sparsity in tabular data.
Our key motivation comes from the recent established connection between
associative memory and attention mechanisms. Consequently, BiSHop uses a
dual-component approach, sequentially processing data both column-wise and
row-wise through two interconnected directional learning modules.
Computationally, these modules house layers of generalized sparse modern
Hopfield layers, a sparse extension of the modern Hopfield model with adaptable
sparsity. Methodologically, BiSHop facilitates multi-scale representation
learning, capturing both intra-feature and inter-feature interactions, with
adaptive sparsity at each scale. Empirically, through experiments on diverse
real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods
with significantly less HPO runs, marking it a robust solution for deep tabular
learning."
48,Outlier-Efficient Hopfield Layers for Large Transformer-Based Models,"We introduce an Outlier-Efficient Modern Hopfield Model (termed
$\mathtt{OutEffHop}$) and use it to address the outlier-induced challenge of
quantizing gigantic transformer-based models. Our main contribution is a novel
associative memory model facilitating \textit{outlier-efficient} associative
memory retrievals. Interestingly, this memory model manifests a model-based
interpretation of an outlier-efficient attention mechanism
($\text{Softmax}_1$): it is an approximation of the memory retrieval process of
$\mathtt{OutEffHop}$. Methodologically, this allows us to debut novel
outlier-efficient Hopfield layers a powerful attention alternative with
superior post-quantization performance. Theoretically, the Outlier-Efficient
Modern Hopfield Model retains and improves the desirable properties of the
standard modern Hopfield models, including fixed point convergence and
exponential storage capacity. Empirically, we demonstrate the proposed model's
efficacy across large-scale transformer-based and Hopfield-based models
(including BERT, OPT, ViT and STanHop-Net), benchmarking against
state-of-the-art methods including $\mathtt{Clipped\_Softmax}$ and
$\mathtt{Gated\_Attention}$. Notably, $\mathtt{OutEffHop}$ achieves on average
$\sim$22+\% reductions in both average kurtosis and maximum infinity norm of
model outputs accross 4 models."
49,Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models,"We propose a two-stage memory retrieval dynamics for modern Hopfield models,
termed $\mathtt{U\text{-}Hop}$, with enhanced memory capacity. Our key
contribution is a learnable feature map $\Phi$ which transforms the Hopfield
energy function into a kernel space. This transformation ensures convergence
between the local minima of energy and the fixed points of retrieval dynamics
within the kernel space. Consequently, the kernel norm induced by $\Phi$ serves
as a novel similarity measure. It utilizes the stored memory patterns as
learning data to enhance memory capacity across all modern Hopfield models.
Specifically, we accomplish this by constructing a separation loss
$\mathcal{L}_\Phi$ that separates the local minima of kernelized energy by
separating stored memory patterns in kernel space. Methodologically,
$\mathtt{U\text{-}Hop}$ memory retrieval process consists of:
\textbf{(Stage~I.)} minimizing separation loss for a more uniformed memory
(local minimum) distribution, followed by \textbf{(Stage~II.)} standard
Hopfield energy minimization for memory retrieval. This results in a
significant reduction of possible meta-stable states in the Hopfield energy
function, thus enhancing memory capacity by preventing memory confusion.
Empirically, with real-world datasets, we demonstrate that
$\mathtt{U\text{-}Hop}$ outperforms all existing modern Hopfield models and
SOTA similarity measures, achieving substantial improvements in both
associative memory retrieval and deep learning tasks."
50,Learning Social Fairness Preferences from Non-Expert Stakeholder Opinions in Kidney Placement,"Modern kidney placement incorporates several intelligent recommendation
systems which exhibit social discrimination due to biases inherited from
training data. Although initial attempts were made in the literature to study
algorithmic fairness in kidney placement, these methods replace true outcomes
with surgeons' decisions due to the long delays involved in recording such
outcomes reliably. However, the replacement of true outcomes with surgeons'
decisions disregards expert stakeholders' biases as well as social opinions of
other stakeholders who do not possess medical expertise. This paper alleviates
the latter concern and designs a novel fairness feedback survey to evaluate an
acceptance rate predictor (ARP) that predicts a kidney's acceptance rate in a
given kidney-match pair. The survey is launched on Prolific, a crowdsourcing
platform, and public opinions are collected from 85 anonymous crowd
participants. A novel social fairness preference learning algorithm is proposed
based on minimizing social feedback regret computed using a novel logit-based
fairness feedback model. The proposed model and learning algorithm are both
validated using simulation experiments as well as Prolific data. Public
preferences towards group fairness notions in the context of kidney placement
have been estimated and discussed in detail. The specific ARP tested in the
Prolific survey has been deemed fair by the participants."
51,Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation,"The increasing relevance of panoptic segmentation is tied to the advancements
in autonomous driving and AR/VR applications. However, the deployment of such
models has been limited due to the expensive nature of dense data annotation,
giving rise to unsupervised domain adaptation (UDA). A key challenge in
panoptic UDA is reducing the domain gap between a labeled source and an
unlabeled target domain while harmonizing the subtasks of semantic and instance
segmentation to limit catastrophic interference. While considerable progress
has been achieved, existing approaches mainly focus on the adaptation of
semantic segmentation. In this work, we focus on incorporating instance-level
adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix
significantly enhances the panoptic quality by improving instance segmentation
performance. Specifically, we propose inserting high-confidence predicted
instances from the target domain onto source images, retaining the
exhaustiveness of the resulting pseudo-labels while reducing the injected
confirmation bias. Nevertheless, such an enhancement comes at the cost of
degraded semantic performance, attributed to catastrophic forgetting. To
mitigate this issue, we regularize our semantic branch by employing CLIP-based
domain alignment (CDA), exploiting the domain-robustness of natural language
prompts. Finally, we present an end-to-end model incorporating these two
mechanisms called LIDAPS, achieving state-of-the-art results on all popular
panoptic UDA benchmarks."
52,Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture,"Safety and robustness are crucial factors in developing trustworthy
autonomous vehicles. One essential aspect of addressing these factors is to
equip vehicles with the capability to predict future trajectories for all
moving objects in the surroundings and quantify prediction uncertainties. In
this paper, we propose the Sequential Neural Variational Agent (SeNeVA), a
generative model that describes the distribution of future trajectories for a
single moving object. Our approach can distinguish Out-of-Distribution data
while quantifying uncertainty and achieving competitive performance compared to
state-of-the-art methods on the Argoverse 2 and INTERACTION datasets.
Specifically, a 0.446 meters minimum Final Displacement Error, a 0.203 meters
minimum Average Displacement Error, and a 5.35% Miss Rate are achieved on the
INTERACTION test set. Extensive qualitative and quantitative analysis is also
provided to evaluate the proposed model. Our open-source code is available at
https://github.com/PurdueDigitalTwin/seneva."
53,Understanding Language Modeling Paradigm Adaptations in Recommender Systems: Lessons Learned and Open Challenges,"The emergence of Large Language Models (LLMs) has achieved tremendous success
in the field of Natural Language Processing owing to diverse training paradigms
that empower LLMs to effectively capture intricate linguistic patterns and
semantic representations. In particular, the recent ""pre-train, prompt and
predict"" training paradigm has attracted significant attention as an approach
for learning generalizable models with limited labeled data. In line with this
advancement, these training paradigms have recently been adapted to the
recommendation domain and are seen as a promising direction in both academia
and industry. This half-day tutorial aims to provide a thorough understanding
of extracting and transferring knowledge from pre-trained models learned
through different training paradigms to improve recommender systems from
various perspectives, such as generality, sparsity, effectiveness and
trustworthiness. In this tutorial, we first introduce the basic concepts and a
generic architecture of the language modeling paradigm for recommendation
purposes. Then, we focus on recent advancements in adapting LLM-related
training strategies and optimization objectives for different recommendation
tasks. After that, we will systematically introduce ethical issues in LLM-based
recommender systems and discuss possible approaches to assessing and mitigating
them. We will also summarize the relevant datasets, evaluation metrics, and an
empirical study on the recommendation performance of training paradigms.
Finally, we will conclude the tutorial with a discussion of open challenges and
future directions."
54,Layerwise Early Stopping for Test Time Adaptation,"Test Time Adaptation (TTA) addresses the problem of distribution shift by
enabling pretrained models to learn new features on an unseen domain at test
time. However, it poses a significant challenge to maintain a balance between
learning new features and retaining useful pretrained features. In this paper,
we propose Layerwise EArly STopping (LEAST) for TTA to address this problem.
The key idea is to stop adapting individual layers during TTA if the features
being learned do not appear beneficial for the new domain. For that purpose, we
propose using a novel gradient-based metric to measure the relevance of the
current learnt features to the new domain without the need for supervised
labels. More specifically, we propose to use this metric to determine
dynamically when to stop updating each layer during TTA. This enables a more
balanced adaptation, restricted to layers benefiting from it, and only for a
certain number of steps. Such an approach also has the added effect of limiting
the forgetting of pretrained features useful for dealing with new domains.
Through extensive experiments, we demonstrate that Layerwise Early Stopping
improves the performance of existing TTA approaches across multiple datasets,
domain shifts, model architectures, and TTA losses."
55,A Systems Theoretic Approach to Online Machine Learning,"The machine learning formulation of online learning is incomplete from a
systems theoretic perspective. Typically, machine learning research emphasizes
domains and tasks, and a problem solving worldview. It focuses on algorithm
parameters, features, and samples, and neglects the perspective offered by
considering system structure and system behavior or dynamics. Online learning
is an active field of research and has been widely explored in terms of
statistical theory and computational algorithms, however, in general, the
literature still lacks formal system theoretical frameworks for modeling online
learning systems and resolving systems-related concept drift issues.
Furthermore, while the machine learning formulation serves to classify methods
and literature, the systems theoretic formulation presented herein serves to
provide a framework for the top-down design of online learning systems,
including a novel definition of online learning and the identification of key
design parameters. The framework is formulated in terms of input-output systems
and is further divided into system structure and system behavior. Concept drift
is a critical challenge faced in online learning, and this work formally
approaches it as part of the system behavior characteristics. Healthcare
provider fraud detection using machine learning is used as a case study
throughout the paper to ground the discussion in a real-world online learning
challenge."
56,A Reinforcement Learning based Reset Policy for CDCL SAT Solvers,"Restart policy is an important technique used in modern Conflict-Driven
Clause Learning (CDCL) solvers, wherein some parts of the solver state are
erased at certain intervals during the run of the solver. In most solvers,
variable activities are preserved across restart boundaries, resulting in
solvers continuing to search parts of the assignment tree that are not far from
the one immediately prior to a restart. To enable the solver to search possibly
""distant"" parts of the assignment tree, we study the effect of resets, a
variant of restarts which not only erases the assignment trail, but also
randomizes the activity scores of the variables of the input formula after
reset, thus potentially enabling a better global exploration of the search
space.
  In this paper, we model the problem of whether to trigger reset as a
multi-armed bandit (MAB) problem, and propose two reinforcement learning (RL)
based adaptive reset policies using the Upper Confidence Bound (UCB) and
Thompson sampling algorithms. These two algorithms balance the
exploration-exploitation tradeoff by adaptively choosing arms (reset vs. no
reset) based on their estimated rewards during the solver's run. We implement
our reset policies in four baseline SOTA CDCL solvers and compare the baselines
against the reset versions on Satcoin benchmarks and SAT Competition instances.
Our results show that RL-based reset versions outperform the corresponding
baseline solvers on both Satcoin and the SAT competition instances, suggesting
that our RL policy helps to dynamically and profitably adapt the reset
frequency for any given input instance. We also introduce the concept of a
partial reset, where at least a constant number of variable activities are
retained across reset boundaries. Building on previous results, we show that
there is an exponential separation between O(1) vs. $\Omega(n)$-length partial
resets."
57,GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation,"Query Reformulation(QR) is a set of techniques used to transform a user's
original search query to a text that better aligns with the user's intent and
improves their search experience. Recently, zero-shot QR has been shown to be a
promising approach due to its ability to exploit knowledge inherent in large
language models. By taking inspiration from the success of ensemble prompting
strategies which have benefited many tasks, we investigate if they can help
improve query reformulation. In this context, we propose an ensemble based
prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot
instruction to generate multiple sets of keywords ultimately improving
retrieval performance. We further introduce its post-retrieval variant,
GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over
four IR benchmarks, we find that GenQREnsemble generates better reformulations
with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over
the previous zero-shot state-of-art. On the MSMarco Passage Ranking task,
GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback,
and 9% nDCG@10 using relevant feedback documents."
58,Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations,"The widespread adoption and transformative effects of large language models
(LLMs) have sparked concerns regarding their capacity to produce inaccurate and
fictitious content, referred to as `hallucinations'. Given the potential risks
associated with hallucinations, humans should be able to identify them. This
research aims to understand the human perception of LLM hallucinations by
systematically varying the degree of hallucination (genuine, minor
hallucination, major hallucination) and examining its interaction with warning
(i.e., a warning of potential inaccuracies: absent vs. present). Participants
(N=419) from Prolific rated the perceived accuracy and engaged with content
(e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank
content as truthful in the order genuine > minor hallucination > major
hallucination and user engagement behaviors mirror this pattern. More
importantly, we observed that warning improves hallucination detection without
significantly affecting the perceived truthfulness of genuine content. We
conclude by offering insights for future tools to aid human detection of
hallucinations."
59,SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection,"We describe the University of Amsterdam Intelligent Data Engineering Lab
team's entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system
builds on previous work on using prompt programming and in-context learning
with large language models (LLMs) to build classifiers for hallucination
detection, and extends that work through the incorporation of context-specific
definition of task, role, and target concept, and automated generation of
examples for use in a few-shot prompting approach. The resulting system
achieved fourth-best and sixth-best performance in the model-agnostic track and
model-aware tracks for Task 6, respectively, and evaluation using the
validation sets showed that the system's classification decisions were
consistent with those of the crowd-sourced human labellers. We further found
that a zero-shot approach provided better accuracy than a few-shot approach
using automatically generated examples. Code for the system described in this
paper is available on Github."
60,OW-VISCap: Open-World Video Instance Segmentation and Captioning,"Open-world video instance segmentation is an important video understanding
task. Yet most methods either operate in a closed-world setting, require an
additional user-input, or use classic region-based proposals to identify never
before seen objects. Further, these methods only assign a one-word label to
detected objects, and don't generate rich object-centric descriptions. They
also often suffer from highly overlapping predictions. To address these issues,
we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),
an approach to jointly segment, track, and caption previously seen or unseen
objects in a video. For this, we introduce open-world object queries to
discover never before seen objects without additional user-input. We generate
rich and descriptive object-centric captions for each detected object via a
masked attention augmented LLM input. We introduce an inter-query contrastive
loss to ensure that the object queries differ from one another. Our generalized
approach matches or surpasses state-of-the-art on three tasks: open-world video
instance segmentation on the BURST dataset, dense video object captioning on
the VidSTG dataset, and closed-world video instance segmentation on the OVIS
dataset."
61,CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching,"Diffusion models have demonstrated great success in the field of
text-to-image generation. However, alleviating the misalignment between the
text prompts and images is still challenging. The root reason behind the
misalignment has not been extensively investigated. We observe that the
misalignment is caused by inadequate token attention activation. We further
attribute this phenomenon to the diffusion model's insufficient condition
utilization, which is caused by its training paradigm. To address the issue, we
propose CoMat, an end-to-end diffusion model fine-tuning strategy with an
image-to-text concept matching mechanism. We leverage an image captioning model
to measure image-to-text alignment and guide the diffusion model to revisit
ignored tokens. A novel attribute concentration module is also proposed to
address the attribute binding problem. Without any image or human preference
data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.
Extensive experiments show that CoMat-SDXL significantly outperforms the
baseline model SDXL in two text-to-image alignment benchmarks and achieves
start-of-the-art performance."
62,AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent,"Large language models (LLMs) have fueled many intelligent agent tasks, such
as web navigation -- but most existing agents perform far from satisfying in
real-world webpages due to three factors: (1) the versatility of actions on
webpages, (2) HTML text exceeding model processing capacity, and (3) the
complexity of decision-making due to the open-domain nature of web. In light of
the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web
navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,
we design an HTML simplification algorithm to represent webpages, preserving
vital information succinctly. We employ a hybrid human-AI method to build web
browsing data for curriculum training. Then, we bootstrap the model by
reinforcement learning and rejection sampling to further facilitate webpage
comprehension, browser operations, and efficient task decomposition by itself.
For testing, we establish a bilingual benchmark -- AutoWebBench -- for
real-world web browsing tasks. We evaluate AutoWebGLM across diverse web
navigation benchmarks, revealing its improvements but also underlying
challenges to tackle real environments. Related code, model, and data will be
released at \url{https://github.com/THUDM/AutoWebGLM}."
63,"Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra","In this paper, we explore the capabilities of state-of-the-art large language
models (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving
undergraduate-level control problems. Controls provides an interesting case
study for LLM reasoning due to its combination of mathematical theory and
engineering design. We introduce ControlBench, a benchmark dataset tailored to
reflect the breadth, depth, and complexity of classical control design. We use
this dataset to study and evaluate the problem-solving abilities of these LLMs
in the context of control engineering. We present evaluations conducted by a
panel of human experts, providing insights into the accuracy, reasoning, and
explanatory prowess of LLMs in control engineering. Our analysis reveals the
strengths and limitations of each LLM in the context of classical control, and
our results imply that Claude 3 Opus has become the state-of-the-art LLM for
solving undergraduate control problems. Our study serves as an initial step
towards the broader goal of employing artificial general intelligence in
control engineering."
64,Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences,"This paper studies post-training large language models (LLMs) using
preference feedback from a powerful oracle to help a model iteratively improve
over itself. The typical approach for post-training LLMs involves Reinforcement
Learning from Human Feedback (RLHF), which traditionally separates reward
learning and subsequent policy optimization. However, such a reward
maximization approach is limited by the nature of ""point-wise"" rewards (such as
Bradley-Terry model), which fails to express complex intransitive or cyclic
preference relations. While advances on RLHF show reward learning and policy
optimization can be merged into a single contrastive objective for stability,
they yet still remain tethered to the reward maximization framework. Recently,
a new wave of research sidesteps the reward maximization presumptions in favor
of directly optimizing over ""pair-wise"" or general preferences. In this paper,
we introduce Direct Nash Optimization (DNO), a provable and scalable algorithm
that marries the simplicity and stability of contrastive learning with
theoretical generality from optimizing general preferences. Because DNO is a
batched on-policy algorithm using a regression-based objective, its
implementation is straightforward and efficient. Moreover, DNO enjoys monotonic
improvement across iterations that help it improve even over a strong teacher
(such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model
aligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of
33% on AlpacaEval 2.0 (even after controlling for response length), an absolute
gain of 26% (7% to 33%) over the initializing model. It outperforms models with
far more parameters, including Mistral Large, Self-Rewarding LM (70B
parameters), and older versions of GPT-4."
65,WorDepth: Variational Language Prior for Monocular Depth Estimation,"Three-dimensional (3D) reconstruction from a single image is an ill-posed
problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text
description(s) is similarly ill-posed, i.e. spatial arrangements of objects
described. We investigate the question of whether two inherently ambiguous
modalities can be used in conjunction to produce metric-scaled reconstructions.
To test this, we focus on monocular depth estimation, the problem of predicting
a dense depth map from a single image, but with an additional text caption
describing the scene. To this end, we begin by encoding the text caption as a
mean and standard deviation; using a variational framework, we learn the
distribution of the plausible metric reconstructions of 3D scenes corresponding
to the text captions as a prior. To ""select"" a specific reconstruction or depth
map, we encode the given image through a conditional sampler that samples from
the latent space of the variational text encoder, which is then decoded to the
output depth map. Our approach is trained alternatingly between the text and
image branches: in one optimization step, we predict the mean and standard
deviation from the text description and sample from a standard Gaussian, and in
the other, we sample using a (image) conditional sampler. Once trained, we
directly predict depth from the encoded text using the conditional sampler. We
demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where
we show that language can consistently improve performance in both."
66,SpikeExplorer: hardware-oriented Design Space Exploration for Spiking Neural Networks on FPGA,"One of today's main concerns is to bring Artificial Intelligence power to
embedded systems for edge applications. The hardware resources and power
consumption required by state-of-the-art models are incompatible with the
constrained environments observed in edge systems, such as IoT nodes and
wearable devices. Spiking Neural Networks (SNNs) can represent a solution in
this sense: inspired by neuroscience, they reach unparalleled power and
resource efficiency when run on dedicated hardware accelerators. However, when
designing such accelerators, the amount of choices that can be taken is huge.
This paper presents SpikExplorer, a modular and flexible Python tool for
hardware-oriented Automatic Design Space Exploration to automate the
configuration of FPGA accelerators for SNNs. Using Bayesian optimizations,
SpikerExplorer enables hardware-centric multi-objective optimization,
supporting factors such as accuracy, area, latency, power, and various
combinations during the exploration process. The tool searches the optimal
network architecture, neuron model, and internal and training parameters,
trying to reach the desired constraints imposed by the user. It allows for a
straightforward network configuration, providing the full set of explored
points for the user to pick the trade-off that best fits the needs. The
potential of SpikExplorer is showcased using three benchmark datasets. It
reaches 95.8% accuracy on the MNIST dataset, with a power consumption of
180mW/image and a latency of 0.12 ms/image, making it a powerful tool for
automatically optimizing SNNs."
67,Standardizing Knowledge Engineering Practices with a Reference Architecture,"Knowledge engineering is the process of creating and maintaining
knowledge-producing systems. Throughout the history of computer science and AI,
knowledge engineering workflows have been widely used given the importance of
high-quality knowledge for reliable intelligent agents. Meanwhile, the scope of
knowledge engineering, as apparent from its target tasks and use cases, has
been shifting, together with its paradigms such as expert systems, semantic
web, and language modeling. The intended use cases and supported user
requirements between these paradigms have not been analyzed globally, as new
paradigms often satisfy prior pain points while possibly introducing new ones.
The recent abstraction of systemic patterns into a boxology provides an opening
for aligning the requirements and use cases of knowledge engineering with the
systems, components, and software that can satisfy them best. This paper
proposes a vision of harmonizing the best practices in the field of knowledge
engineering by leveraging the software engineering methodology of creating
reference architectures. We describe how a reference architecture can be
iteratively designed and implemented to associate user needs with recurring
systemic patterns, building on top of existing knowledge engineering workflows
and boxologies. We provide a six-step roadmap that can enable the development
of such an architecture, providing an initial design and outcome of the
definition of architectural scope, selection of information sources, and
analysis. We expect that following through on this vision will lead to
well-grounded reference architectures for knowledge engineering, will advance
the ongoing initiatives of organizing the neurosymbolic knowledge engineering
space, and will build new links to the software architectures and data science
communities."
68,Explaining Explainability: Understanding Concept Activation Vectors,"Recent interpretability methods propose using concept-based explanations to
translate the internal representations of deep learning models into a language
that humans are familiar with: concepts. This requires understanding which
concepts are present in the representation space of a neural network. One
popular method for finding concepts is Concept Activation Vectors (CAVs), which
are learnt using a probe dataset of concept exemplars. In this work, we
investigate three properties of CAVs. CAVs may be: (1) inconsistent between
layers, (2) entangled with different concepts, and (3) spatially dependent.
Each property provides both challenges and opportunities in interpreting
models. We introduce tools designed to detect the presence of these properties,
provide insight into how they affect the derived explanations, and provide
recommendations to minimise their impact. Understanding these properties can be
used to our advantage. For example, we introduce spatially dependent CAVs to
test if a model is translation invariant with respect to a specific concept and
class. Our experiments are performed on ImageNet and a new synthetic dataset,
Elements. Elements is designed to capture a known ground truth relationship
between concepts and classes. We release this dataset to facilitate further
research in understanding and evaluating interpretability methods."
69,Unveiling LLMs: The Evolution of Latent Representations in a Temporal Knowledge Graph,"Large Language Models (LLMs) demonstrate an impressive capacity to recall a
vast range of common factual knowledge information. However, unravelling the
underlying reasoning of LLMs and explaining their internal mechanisms of
exploiting this factual knowledge remain active areas of investigation. Our
work analyzes the factual knowledge encoded in the latent representation of
LLMs when prompted to assess the truthfulness of factual claims. We propose an
end-to-end framework that jointly decodes the factual knowledge embedded in the
latent space of LLMs from a vector space to a set of ground predicates and
represents its evolution across the layers using a temporal knowledge graph.
Our framework relies on the technique of activation patching which intervenes
in the inference computation of a model by dynamically altering its latent
representations. Consequently, we neither rely on external models nor training
processes. We showcase our framework with local and global interpretability
analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The
local interpretability analysis exposes different latent errors from
representation to multi-hop reasoning errors. On the other hand, the global
analysis uncovered patterns in the underlying evolution of the model's factual
knowledge (e.g., store-and-seek factual information). By enabling graph-based
analyses of the latent representations, this work represents a step towards the
mechanistic interpretability of LLMs."
70,InsectMamba: Insect Pest Classification with State Space Model,"The classification of insect pests is a critical task in agricultural
technology, vital for ensuring food security and environmental sustainability.
However, the complexity of pest identification, due to factors like high
camouflage and species diversity, poses significant obstacles. Existing methods
struggle with the fine-grained feature extraction needed to distinguish between
closely related pest species. Although recent advancements have utilized
modified network structures and combined deep learning approaches to improve
accuracy, challenges persist due to the similarity between pests and their
surroundings. To address this problem, we introduce InsectMamba, a novel
approach that integrates State Space Models (SSMs), Convolutional Neural
Networks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer
Perceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the
extraction of comprehensive visual features by leveraging the strengths of each
encoding strategy. A selective module is also proposed to adaptively aggregate
these features, enhancing the model's ability to discern pest characteristics.
InsectMamba was evaluated against strong competitors across five insect pest
classification datasets. The results demonstrate its superior performance and
verify the significance of each model component by an ablation study."
71,Sailor: Open Language Models for South-East Asia,"We present Sailor, a family of open language models ranging from 0.5B to 7B
parameters, tailored for South-East Asian (SEA) languages. These models are
continually pre-trained from Qwen1.5, a great language model for multilingual
use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily
covering the languages of English, Chinese, Vietnamese, Thai, Indonesian,
Malay, and Lao. The training leverages several techniques, including BPE
dropout for improving the model robustness, aggressive data cleaning and
deduplication, and small proxy models to optimize data mixture. Experimental
results on four typical tasks indicate that Sailor models demonstrate strong
performance across different benchmarks, including commonsense reasoning,
question answering, reading comprehension and examination. Embracing the
open-source spirit, we share our insights through this report to spark a wider
interest in developing large language models for multilingual use cases."
72,Analyzing Musical Characteristics of National Anthems in Relation to Global Indices,"Music plays a huge part in shaping peoples' psychology and behavioral
patterns. This paper investigates the connection between national anthems and
different global indices with computational music analysis and statistical
correlation analysis. We analyze national anthem musical data to determine
whether certain musical characteristics are associated with peace, happiness,
suicide rate, crime rate, etc. To achieve this, we collect national anthems
from 169 countries and use computational music analysis techniques to extract
pitch, tempo, beat, and other pertinent audio features. We then compare these
musical characteristics with data on different global indices to ascertain
whether a significant correlation exists. Our findings indicate that there may
be a correlation between the musical characteristics of national anthems and
the indices we investigated. The implications of our findings for music
psychology and policymakers interested in promoting social well-being are
discussed. This paper emphasizes the potential of musical data analysis in
social research and offers a novel perspective on the relationship between
music and social indices. The source code and data are made open-access for
reproducibility and future research endeavors. It can be accessed at
http://bit.ly/na_code."
73,Intent Detection and Entity Extraction from BioMedical Literature,"Biomedical queries have become increasingly prevalent in web searches,
reflecting the growing interest in accessing biomedical literature. Despite
recent research on large-language models (LLMs) motivated by endeavours to
attain generalized intelligence, their efficacy in replacing task and
domain-specific natural language understanding approaches remains questionable.
In this paper, we address this question by conducting a comprehensive empirical
evaluation of intent detection and named entity recognition (NER) tasks from
biomedical text. We show that Supervised Fine Tuned approaches are still
relevant and more effective than general-purpose LLMs. Biomedical transformer
models such as PubMedBERT can surpass ChatGPT on NER task with only 5
supervised examples."
74,Laser Learning Environment: A new environment for coordination-critical multi-agent tasks,"We introduce the Laser Learning Environment (LLE), a collaborative
multi-agent reinforcement learning environment in which coordination is
central. In LLE, agents depend on each other to make progress
(interdependence), must jointly take specific sequences of actions to succeed
(perfect coordination), and accomplishing those joint actions does not yield
any intermediate reward (zero-incentive dynamics). The challenge of such
problems lies in the difficulty of escaping state space bottlenecks caused by
interdependence steps since escaping those bottlenecks is not rewarded. We test
multiple state-of-the-art value-based MARL algorithms against LLE and show that
they consistently fail at the collaborative task because of their inability to
escape state space bottlenecks, even though they successfully achieve perfect
coordination. We show that Q-learning extensions such as prioritized experience
replay and n-steps return hinder exploration in environments with
zero-incentive dynamics, and find that intrinsic curiosity with random network
distillation is not sufficient to escape those bottlenecks. We demonstrate the
need for novel methods to solve this problem and the relevance of LLE as
cooperative MARL benchmark."
75,ReFT: Representation Finetuning for Language Models,"Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via
updates to a small number of weights. However, much prior interpretability work
has shown that representations encode rich semantic information, suggesting
that editing representations might be a more powerful alternative. Here, we
pursue this hypothesis by developing a family of $\textbf{Representation
Finetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and
learn task-specific interventions on hidden representations. We define a strong
instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is
a drop-in replacement for existing PEFTs and learns interventions that are
10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase
LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,
Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best
balance of efficiency and performance, and almost always outperforms
state-of-the-art PEFTs. We release a generic ReFT training library publicly at
https://github.com/stanfordnlp/pyreft."
76,SemGrasp: Semantic Grasp Generation via Language Aligned Discretization,"Generating natural human grasps necessitates consideration of not just object
geometry but also semantic information. Solely depending on object shape for
grasp generation confines the applications of prior methods in downstream
tasks. This paper presents a novel semantic-based grasp generation method,
termed SemGrasp, which generates a static human grasp pose by incorporating
semantic information into the grasp representation. We introduce a discrete
representation that aligns the grasp space with semantic space, enabling the
generation of grasp postures in accordance with language instructions. A
Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating
object, grasp, and language within a unified semantic space. To facilitate the
training of SemGrasp, we have compiled a large-scale, grasp-text-aligned
dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse
grasps. Experimental findings demonstrate that SemGrasp efficiently generates
natural human grasps in alignment with linguistic intentions. Our code, models,
and dataset are available publicly at: https://kailinli.github.io/SemGrasp."
77,Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration,"An agent assisting humans in daily living activities can collaborate more
effectively by anticipating upcoming tasks. Data-driven methods represent the
state of the art in task anticipation, planning, and related problems, but
these methods are resource-hungry and opaque. Our prior work introduced a proof
of concept framework that used an LLM to anticipate 3 high-level tasks that
served as goals for a classical planning system that computed a sequence of
low-level actions for the agent to achieve these goals. This paper describes
DaTAPlan, our framework that significantly extends our prior work toward
human-robot collaboration. Specifically, DaTAPlan planner computes actions for
an agent and a human to collaboratively and jointly achieve the tasks
anticipated by the LLM, and the agent automatically adapts to unexpected
changes in human action outcomes and preferences. We evaluate DaTAPlan
capabilities in a realistic simulation environment, demonstrating accurate task
anticipation, effective human-robot collaboration, and the ability to adapt to
unexpected changes. Project website: https://dataplan-hrc.github.io"
78,TinyVQA: Compact Multimodal Deep Neural Network for Visual Question Answering on Resource-Constrained Devices,"Traditional machine learning models often require powerful hardware, making
them unsuitable for deployment on resource-limited devices. Tiny Machine
Learning (tinyML) has emerged as a promising approach for running machine
learning models on these devices, but integrating multiple data modalities into
tinyML models still remains a challenge due to increased complexity, latency,
and power consumption. This paper proposes TinyVQA, a novel multimodal deep
neural network for visual question answering tasks that can be deployed on
resource-constrained tinyML hardware. TinyVQA leverages a supervised
attention-based model to learn how to answer questions about images using both
vision and language modalities. Distilled knowledge from the supervised
attention-based VQA model trains the memory aware compact TinyVQA model and low
bit-width quantization technique is employed to further compress the model for
deployment on tinyML devices. The TinyVQA model was evaluated on the FloodNet
dataset, which is used for post-disaster damage assessment. The compact model
achieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for
real-world applications. Additionally, the model was deployed on a Crazyflie
2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA model
achieved low latencies of 56 ms and consumes 693 mW power while deployed on the
tiny drone, showcasing its suitability for resource-constrained embedded
systems."
79,Alzheimer's disease detection in PSG signals,"Alzheimer's disease (AD) and sleep disorders exhibit a close association,
where disruptions in sleep patterns often precede the onset of Mild Cognitive
Impairment (MCI) and early-stage AD. This study delves into the potential of
utilizing sleep-related electroencephalography (EEG) signals acquired through
polysomnography (PSG) for the early detection of AD. Our primary focus is on
exploring semi-supervised Deep Learning techniques for the classification of
EEG signals due to the clinical scenario characterized by the limited data
availability. The methodology entails testing and comparing the performance of
semi-supervised SMATE and TapNet models, benchmarked against the supervised XCM
model, and unsupervised Hidden Markov Models (HMMs). The study highlights the
significance of spatial and temporal analysis capabilities, conducting
independent analyses of each sleep stage. Results demonstrate the effectiveness
of SMATE in leveraging limited labeled data, achieving stable metrics across
all sleep stages, and reaching 90% accuracy in its supervised form. Comparative
analyses reveal SMATE's superior performance over TapNet and HMM, while XCM
excels in supervised scenarios with an accuracy range of 92 - 94%. These
findings underscore the potential of semi-supervised models in early AD
detection, particularly in overcoming the challenges associated with the
scarcity of labeled data. Ablation tests affirm the critical role of
spatio-temporal feature extraction in semi-supervised predictive performance,
and t-SNE visualizations validate the model's proficiency in distinguishing AD
patterns. Overall, this research contributes to the advancement of AD detection
through innovative Deep Learning approaches, highlighting the crucial role of
semi-supervised learning in addressing data limitations."
80,CodeEditorBench: Evaluating Code Editing Capability of Large Language Models,"Large Language Models (LLMs) for code are rapidly evolving, with code editing
emerging as a critical capability. We introduce CodeEditorBench, an evaluation
framework designed to rigorously assess the performance of LLMs in code editing
tasks, including debugging, translating, polishing, and requirement switching.
Unlike existing benchmarks focusing solely on code generation, CodeEditorBench
emphasizes real-world scenarios and practical aspects of software development.
We curate diverse coding challenges and scenarios from five sources, covering
various programming languages, complexity levels, and editing tasks. Evaluation
of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and
GPT-4), outperform open-source models in CodeEditorBench, highlighting
differences in model performance based on problem types and prompt
sensitivities. CodeEditorBench aims to catalyze advancements in LLMs by
providing a robust platform for assessing code editing capabilities. We will
release all prompts and datasets to enable the community to expand the dataset
and benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to
the advancement of LLMs in code editing and provide a valuable resource for
researchers and practitioners."
81,AMC-backed Twin Arrow Antenna for Wearable Electronic Travel Aid System at 24 GHz,"An ultra-compact wearable antenna, for electronic travel aid (ETA)
applications, is presented. An AMC-backed twin arrow antenna, operative in the
24.05-24.25 GHz frequency band, has been designed for imaging systems
supporting ETA. Artificial Magnetic Conductor (AMC) is combined with the
antenna with the aim of reducing the backward radiation to the wearing person
while improving its radiation properties and bandwidth, all this without
increasing the initial area of the antenna. Prototypes of the AMC-antenna have
been fabricated and measured. In order to test its performance for the
application, imaging have been conducted by means of synthetic aperture radar
(SAR) techniques by placing the antenna in the arm of a user to take advantage
of natural body movement. Electromagnetic images have been obtained and the
target has been identified, demonstrating the suitability of the AMC-antenna
for the ETA system."
82,Integrating Generative AI into Financial Market Prediction for Improved Decision Making,"This study provides an in-depth analysis of the model architecture and key
technologies of generative artificial intelligence, combined with specific
application cases, and uses conditional generative adversarial networks ( cGAN
) and time series analysis methods to simulate and predict dynamic changes in
financial markets. The research results show that the cGAN model can
effectively capture the complexity of financial market data, and the deviation
between the prediction results and the actual market performance is minimal,
showing a high degree of accuracy."
83,Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach,"Retrieval-augmented large language models (LLMs) have been remarkably
competent in various NLP tasks. Despite their great success, the knowledge
provided by the retrieval process is not always useful for improving the model
prediction, since in some samples LLMs may already be quite knowledgeable and
thus be able to answer the question correctly without retrieval. Aiming to save
the cost of retrieval, previous work has proposed to determine when to do/skip
the retrieval in a data-aware manner by analyzing the LLMs' pretraining data.
However, these data-aware methods pose privacy risks and memory limitations,
especially when requiring access to sensitive or extensive pretraining data.
Moreover, these methods offer limited adaptability under fine-tuning or
continual learning settings. We hypothesize that token embeddings are able to
capture the model's intrinsic knowledge, which offers a safer and more
straightforward way to judge the need for retrieval without the privacy risks
associated with accessing pre-training data. Moreover, it alleviates the need
to retain all the data utilized during model pre-training, necessitating only
the upkeep of the token embeddings. Extensive experiments and in-depth analyses
demonstrate the superiority of our model-aware approach."
84,Privacy-Enhancing Technologies for Artificial Intelligence-Enabled Systems,"Artificial intelligence (AI) models introduce privacy vulnerabilities to
systems. These vulnerabilities may impact model owners or system users; they
exist during model development, deployment, and inference phases, and threats
can be internal or external to the system. In this paper, we investigate
potential threats and propose the use of several privacy-enhancing technologies
(PETs) to defend AI-enabled systems. We then provide a framework for PETs
evaluation for a AI-enabled systems and discuss the impact PETs may have on
system-level variables."
85,AI and the Problem of Knowledge Collapse,"While artificial intelligence has the potential to process vast amounts of
data, generate new insights, and unlock greater productivity, its widespread
adoption may entail unforeseen consequences. We identify conditions under which
AI, by reducing the cost of access to certain modes of knowledge, can
paradoxically harm public understanding. While large language models are
trained on vast amounts of diverse data, they naturally generate output towards
the 'center' of the distribution. This is generally useful, but widespread
reliance on recursive AI systems could lead to a process we define as
""knowledge collapse"", and argue this could harm innovation and the richness of
human understanding and culture. However, unlike AI models that cannot choose
what data they are trained on, humans may strategically seek out diverse forms
of knowledge if they perceive them to be worthwhile. To investigate this, we
provide a simple model in which a community of learners or innovators choose to
use traditional methods or to rely on a discounted AI-assisted process and
identify conditions under which knowledge collapse occurs. In our default
model, a 20% discount on AI-generated content generates public beliefs 2.3
times further from the truth than when there is no discount. Finally, based on
the results, we consider further research directions to counteract such
outcomes."
86,Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research."
87,A Methodology to Study the Impact of Spiking Neural Network Parameters considering Event-Based Automotive Data,"Autonomous Driving (AD) systems are considered as the future of human
mobility and transportation. Solving computer vision tasks such as image
classification and object detection/segmentation, with high accuracy and low
power/energy consumption, is highly needed to realize AD systems in real life.
These requirements can potentially be satisfied by Spiking Neural Networks
(SNNs). However, the state-of-the-art works in SNN-based AD systems still focus
on proposing network models that can achieve high accuracy, and they have not
systematically studied the roles of SNN parameters when used for learning
event-based automotive data. Therefore, we still lack understanding of how to
effectively develop SNN models for AD systems. Toward this, we propose a novel
methodology to systematically study and analyze the impact of SNN parameters
considering event-based automotive data, then leverage this analysis for
enhancing SNN developments. To do this, we first explore different settings of
SNN parameters that directly affect the learning mechanism (i.e., batch size,
learning rate, neuron threshold potential, and weight decay), then analyze the
accuracy results. Afterward, we propose techniques that jointly improve SNN
accuracy and reduce training time. Experimental results show that our
methodology can improve the SNN models for AD systems than the
state-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS
dataset, and it can also achieve iso-accuracy (i.e., ~85% with standard
deviation less than 0.5%) while speeding up the training time by 1.9x. In this
manner, our research work provides a set of guidelines for SNN parameter
enhancements, thereby enabling the practical developments of SNN-based AD
systems."
88,A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation,"Empowered by the large-scale pretrained language models, existing dialogue
systems have demonstrated impressive performance conducting fluent and
natural-sounding conversations. However, they are still plagued by the
hallucination problem, causing unpredictable factual errors in the generated
responses. Recently, knowledge-grounded dialogue generation models, that
intentionally invoke external knowledge resources to more informative
responses, are also proven to be effective in reducing hallucination. Following
the idea of getting high-quality knowledge, a few efforts have achieved pretty
good performance on this issue. As some inevitable knowledge noises may also
lead to hallucinations, it is emergent to investigate the reason and future
directions for building noise-tolerant methods in KGD tasks. In this paper, we
analyze the causal story behind this problem with counterfactual reasoning
methods. Based on the causal effect analysis, we propose a possible solution
for alleviating the hallucination in KGD by exploiting the dialogue-knowledge
interaction. Experimental results of our example implementation show that this
method can reduce hallucination without disrupting other dialogue performance,
while keeping adaptive to different generation models. We hope our efforts can
support and call for more attention to developing lightweight techniques
towards robust and trusty dialogue systems."
89,Generative AI and Teachers -- For Us or Against Us? A Case Study,"We present insightful results of a survey on the adoption of generative
artificial intelligence (GenAI) by university teachers in their teaching
activities. The transformation of education by GenAI, particularly large
language models (LLMs), has been presenting both opportunities and challenges,
including cheating by students. We prepared the online survey according to best
practices and the questions were created by the authors, who have pedagogy
experience. The survey contained 12 questions and a pilot study was first
conducted. The survey was then sent to all teachers in multiple departments
across different campuses of the university of interest in Sweden: Lule{\aa}
University of Technology. The survey was available in both Swedish and English.
The results show that 35 teachers (more than half) use GenAI out of 67
respondents. Preparation is the teaching activity with the most frequency that
GenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it has
impacted their teaching, however, 55% say there should be legislation around
the use of GenAI, especially as inaccuracies and cheating are the biggest
concerns."
90,Performance of computer vision algorithms for fine-grained classification using crowdsourced insect images,"With fine-grained classification, we identify unique characteristics to
distinguish among classes of the same super-class. We are focusing on species
recognition in Insecta, as they are critical for biodiversity monitoring and at
the base of many ecosystems. With citizen science campaigns, billions of images
are collected in the wild. Once these are labelled, experts can use them to
create distribution maps. However, the labelling process is time-consuming,
which is where computer vision comes in. The field of computer vision offers a
wide range of algorithms, each with its strengths and weaknesses; how do we
identify the algorithm that is in line with our application? To answer this
question, we provide a full and detailed evaluation of nine algorithms among
deep convolutional networks (CNN), vision transformers (ViT), and
locality-based vision transformers (LBVT) on 4 different aspects:
classification performance, embedding quality, computational cost, and gradient
activity. We offer insights that we haven't yet had in this domain proving to
which extent these algorithms solve the fine-grained tasks in Insecta. We found
that the ViT performs the best on inference speed and computational cost while
the LBVT outperforms the others on performance and embedding quality; the CNN
provide a trade-off among the metrics."
91,Reevaluating Bias Detection in Language Models: The Role of Implicit Norm,"Large language models (LLMs), trained on vast datasets, can carry biases that
manifest in various forms, from overt discrimination to implicit stereotypes.
One facet of bias is performance disparities in LLMs, often harming
underprivileged groups, such as racial minorities. A common approach to
quantifying bias is to use template-based bias probes, which explicitly state
group membership (e.g. White) and evaluate if the outcome of a task, sentiment
analysis for instance, is invariant to the change of group membership (e.g.
change White race to Black). This approach is widely used in bias
quantification. However, in this work, we find evidence of an unexpectedly
overlooked consequence of using template-based probes for LLM bias
quantification. We find that in doing so, text examples associated with White
ethnicities appear to be classified as exhibiting negative sentiment at
elevated rates. We hypothesize that the scenario arises artificially through a
mismatch between the pre-training text of LLMs and the templates used to
measure bias through reporting bias, unstated norms that imply group membership
without explicit statement. Our finding highlights the potential misleading
impact of varying group membership through explicit mention in bias
quantification"
92,"Integrating AI in NDE: Techniques, Trends, and Further Directions","The digital transformation is fundamentally changing our industries,
affecting planning, execution as well as monitoring of production processes in
a wide range of application fields. With product line-ups becoming more and
more versatile and diverse, the necessary inspection and monitoring sparks
significant novel requirements on the corresponding Nondestructive Evaluation
(NDE) systems. The establishment of increasingly powerful approaches to
incorporate Artificial Intelligence (AI) may provide just the needed innovation
to solve some of these challenges.
  In this paper we provide a comprehensive survey about the usage of AI methods
in NDE in light of the recent innovations towards NDE 4.0. Since we cannot
discuss each NDE modality in one paper, we limit our attention to magnetic
methods, ultrasound, thermography, as well as optical inspection. In addition
to reviewing recent AI developments in each field, we draw common connections
by pointing out NDE-related tasks that have a common underlying mathematical
problem and categorizing the state of the art according to the corresponding
sub-tasks. In so doing, interdisciplinary connections are drawn that provide a
more complete overall picture."
93,ZynqMP-based board-management mezzanines for Serenity ATCA-blades,"In the context of the CMS Phase-2 tracker back-end processing system, two
mezzanines based on the Zynq Ultrascale+ Multi-Processor System-on-Chip (MPSoC)
device have been developed to serve as centralized slow control and board
management solution for the Serenity-family \textcolor{black}{Advanced
Telecommunications Computing Architecture (ATCA)} blades.
  This paper presents the developments of the MPSoC mezzanines to execute the
Intelligent Platform Management Controller (IPMC) software in the real-time
capable processors of the MPSoC. In coordination with the Shelf Manager, once
full-power is enabled, a CentOS-based Linux distribution is executed in the
application processors of the MPSoC, on which EMPButler and the Serenity
Management Shell (SMASH) are running."
94,Self-organized arrival system for urban air mobility,"Urban air mobility is an innovative mode of transportation in which electric
vertical takeoff and landing (eVTOL) vehicles operate between nodes called
vertiports. We outline a self-organized vertiport arrival system based on deep
reinforcement learning. The airspace around the vertiport is assumed to be
circular, and the vehicles can freely operate inside. Each aircraft is
considered an individual agent and follows a shared policy, resulting in
decentralized actions that are based on local information. We investigate the
development of the reinforcement learning policy during training and illustrate
how the algorithm moves from suboptimal local holding patterns to a safe and
efficient final policy. The latter is validated in simulation-based scenarios
and also deployed on small-scale unmanned aerial vehicles to showcase its
real-world usability."
95,Benchmarking ChatGPT on Algorithmic Reasoning,"We evaluate ChatGPT's ability to solve algorithm problems from the CLRS
benchmark suite that is designed for GNNs. The benchmark requires the use of a
specified classical algorithm to solve a given problem. We find that ChatGPT
outperforms specialist GNN models, using Python to successfully solve these
problems. This raises new points in the discussion about learning algorithms
with neural networks."
96,Interpreting End-to-End Deep Learning Models for Speech Source Localization Using Layer-wise Relevance Propagation,"Deep learning models are widely applied in the signal processing community,
yet their inner working procedure is often treated as a black box. In this
paper, we investigate the use of eXplainable Artificial Intelligence (XAI)
techniques to learning-based end-to-end speech source localization models. We
consider the Layer-wise Relevance Propagation (LRP) technique, which aims to
determine which parts of the input are more important for the output
prediction. Using LRP we analyze two state-of-the-art models, of differing
architectural complexity that map audio signals acquired by the microphones to
the cartesian coordinates of the source. Specifically, we inspect the relevance
associated with the input features of the two models and discover that both
networks denoise and de-reverberate the microphone signals to compute more
accurate statistical correlations between them and consequently localize the
sources. To further demonstrate this fact, we estimate the Time-Difference of
Arrivals (TDoAs) via the Generalized Cross Correlation with Phase Transform
(GCC-PHAT) using both microphone signals and relevance signals extracted from
the two networks and show that through the latter we obtain more accurate
time-delay estimation results."
97,Scaffolding Language Learning via Multi-modal Tutoring Systems with Pedagogical Instructions,"Intelligent tutoring systems (ITSs) that imitate human tutors and aim to
provide immediate and customized instructions or feedback to learners have
shown their effectiveness in education. With the emergence of generative
artificial intelligence, large language models (LLMs) further entitle the
systems to complex and coherent conversational interactions. These systems
would be of great help in language education as it involves developing skills
in communication, which, however, drew relatively less attention. Additionally,
due to the complicated cognitive development at younger ages, more endeavors
are needed for practical uses. Scaffolding refers to a teaching technique where
teachers provide support and guidance to students for learning and developing
new concepts or skills. It is an effective way to support diverse learning
needs, goals, processes, and outcomes. In this work, we investigate how
pedagogical instructions facilitate the scaffolding in ITSs, by conducting a
case study on guiding children to describe images for language learning. We
construct different types of scaffolding tutoring systems grounded in four
fundamental learning theories: knowledge construction, inquiry-based learning,
dialogic teaching, and zone of proximal development. For qualitative and
quantitative analyses, we build and refine a seven-dimension rubric to evaluate
the scaffolding process. In our experiment on GPT-4V, we observe that LLMs
demonstrate strong potential to follow pedagogical instructions and achieve
self-paced learning in different student groups. Moreover, we extend our
evaluation framework from a manual to an automated approach, paving the way to
benchmark various conversational tutoring systems."
98,ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model,"Convolutional neural networks (CNN) and Transformers have made impressive
progress in the field of remote sensing change detection (CD). However, both
architectures have their inherent shortcomings. Recently, the Mamba
architecture, based on spatial state models, has shown remarkable performance
in a series of natural language processing tasks, which can effectively
compensate for the shortcomings of the above two architectures. In this paper,
we explore for the first time the potential of the Mamba architecture for
remote sensing change detection tasks. We tailor the corresponding frameworks,
called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD),
semantic change detection (SCD), and building damage assessment (BDA),
respectively. All three frameworks adopt the cutting-edge visual Mamba
architecture as the encoder, which allows full learning of global spatial
contextual information from the input images. For the change decoder, which is
available in all three architectures, we propose three spatio-temporal
relationship modeling mechanisms, which can be naturally combined with the
Mamba architecture and fully utilize its attribute to achieve spatio-temporal
interaction of multi-temporal features and obtain accurate change information.
On five benchmark datasets, our proposed frameworks outperform current CNN- and
Transformer-based approaches without using any complex strategies or tricks,
fully demonstrating the potential of the Mamba architecture. Specifically, we
obtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU,
LEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and
on the xBD dataset, we obtained 81.41% overall F1 score. The source code will
be available in https://github.com/ChenHongruixuan/MambaCD"
99,Integrating Hyperparameter Search into GramML,"Automated Machine Learning (AutoML) has become increasingly popular in recent
years due to its ability to reduce the amount of time and expertise required to
design and develop machine learning systems. This is very important for the
practice of machine learning, as it allows building strong baselines quickly,
improving the efficiency of the data scientists, and reducing the time to
production. However, despite the advantages of AutoML, it faces several
challenges, such as defining the solutions space and exploring it efficiently.
Recently, some approaches have been shown to be able to do it using tree-based
search algorithms and context-free grammars. In particular, GramML presents a
model-free reinforcement learning approach that leverages pipeline
configuration grammars and operates using Monte Carlo tree search. However, one
of the limitations of GramML is that it uses default hyperparameters, limiting
the search problem to finding optimal pipeline structures for the available
data preprocessors and models. In this work, we propose an extension to GramML
that supports larger search spaces including hyperparameter search. We
evaluated the approach using an OpenML benchmark and found significant
improvements compared to other state-of-the-art techniques."
